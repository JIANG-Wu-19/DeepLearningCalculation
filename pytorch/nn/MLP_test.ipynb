{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from MLP import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=torchvision.datasets.CIFAR10(root=\"../data\",train=True,transform=transforms.ToTensor(),download=False)\n",
    "test_set=torchvision.datasets.CIFAR10(root=\"../data\",train=False,transform=transforms.ToTensor(),download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "lr=0.01\n",
    "epoch=20\n",
    "writer=SummaryWriter(\"logs\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trainLoader=DataLoader(train_set,batch_size=batch_size,shuffle=True,num_workers=0,drop_last=False)\n",
    "testLoader=DataLoader(test_set,batch_size=batch_size,shuffle=True,num_workers=0,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLP(3072)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainLoader,model,loss_fn,optimizer):\n",
    "    print(f\"training on:{device}\")\n",
    "    model=model.to(device)\n",
    "    size=len(trainLoader.dataset)\n",
    "    \n",
    "    for batch,(X,y) in enumerate(trainLoader):\n",
    "        X=X.to(device)\n",
    "        y=y.to(device)\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred,y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch%10==0:\n",
    "            loss,current=loss.item(),batch*len(X)\n",
    "            print(f\"loss:{loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_correct=0\n",
    "def test(testLoader, model, loss_fn,max_correct):\n",
    "    model = model.to(device)\n",
    "    size = len(testLoader.dataset)\n",
    "    num_batches = len(testLoader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in testLoader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    correct /= size\n",
    "    \n",
    "    if correct>max_correct:\n",
    "        torch.save(model.state_dict(),'mlp.params')\n",
    "        max_correct=correct\n",
    "\n",
    "    print(f\"test error:\\n accuracy:{(100 * correct):>0.1f}%,avg loss: {test_loss:>8f} \\n\")\n",
    "    return max_correct,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.304476 [    0/50000]\n",
      "loss:2.304917 [  640/50000]\n",
      "loss:2.284623 [ 1280/50000]\n",
      "loss:2.283258 [ 1920/50000]\n",
      "loss:2.282411 [ 2560/50000]\n",
      "loss:2.288828 [ 3200/50000]\n",
      "loss:2.296762 [ 3840/50000]\n",
      "loss:2.292325 [ 4480/50000]\n",
      "loss:2.275413 [ 5120/50000]\n",
      "loss:2.296201 [ 5760/50000]\n",
      "loss:2.295669 [ 6400/50000]\n",
      "loss:2.271317 [ 7040/50000]\n",
      "loss:2.261642 [ 7680/50000]\n",
      "loss:2.270866 [ 8320/50000]\n",
      "loss:2.256522 [ 8960/50000]\n",
      "loss:2.243615 [ 9600/50000]\n",
      "loss:2.222149 [10240/50000]\n",
      "loss:2.249742 [10880/50000]\n",
      "loss:2.223388 [11520/50000]\n",
      "loss:2.242452 [12160/50000]\n",
      "loss:2.261486 [12800/50000]\n",
      "loss:2.233497 [13440/50000]\n",
      "loss:2.195861 [14080/50000]\n",
      "loss:2.261210 [14720/50000]\n",
      "loss:2.177195 [15360/50000]\n",
      "loss:2.225741 [16000/50000]\n",
      "loss:2.180796 [16640/50000]\n",
      "loss:2.224156 [17280/50000]\n",
      "loss:2.227991 [17920/50000]\n",
      "loss:2.139794 [18560/50000]\n",
      "loss:2.184254 [19200/50000]\n",
      "loss:2.177902 [19840/50000]\n",
      "loss:2.148236 [20480/50000]\n",
      "loss:2.163007 [21120/50000]\n",
      "loss:2.240342 [21760/50000]\n",
      "loss:2.140638 [22400/50000]\n",
      "loss:2.103302 [23040/50000]\n",
      "loss:2.053733 [23680/50000]\n",
      "loss:2.053109 [24320/50000]\n",
      "loss:2.114202 [24960/50000]\n",
      "loss:2.093428 [25600/50000]\n",
      "loss:2.032048 [26240/50000]\n",
      "loss:2.048516 [26880/50000]\n",
      "loss:2.095500 [27520/50000]\n",
      "loss:2.019703 [28160/50000]\n",
      "loss:2.016925 [28800/50000]\n",
      "loss:1.943162 [29440/50000]\n",
      "loss:2.139788 [30080/50000]\n",
      "loss:2.078654 [30720/50000]\n",
      "loss:2.049676 [31360/50000]\n",
      "loss:2.026051 [32000/50000]\n",
      "loss:2.027481 [32640/50000]\n",
      "loss:2.224221 [33280/50000]\n",
      "loss:2.078812 [33920/50000]\n",
      "loss:2.001749 [34560/50000]\n",
      "loss:2.004798 [35200/50000]\n",
      "loss:2.067637 [35840/50000]\n",
      "loss:2.028464 [36480/50000]\n",
      "loss:2.135269 [37120/50000]\n",
      "loss:2.014360 [37760/50000]\n",
      "loss:1.982851 [38400/50000]\n",
      "loss:2.060209 [39040/50000]\n",
      "loss:2.001715 [39680/50000]\n",
      "loss:2.048551 [40320/50000]\n",
      "loss:1.942488 [40960/50000]\n",
      "loss:2.018330 [41600/50000]\n",
      "loss:1.934524 [42240/50000]\n",
      "loss:1.945150 [42880/50000]\n",
      "loss:2.032147 [43520/50000]\n",
      "loss:2.030848 [44160/50000]\n",
      "loss:1.892227 [44800/50000]\n",
      "loss:1.793550 [45440/50000]\n",
      "loss:1.943708 [46080/50000]\n",
      "loss:1.981738 [46720/50000]\n",
      "loss:1.902910 [47360/50000]\n",
      "loss:1.918637 [48000/50000]\n",
      "loss:2.005990 [48640/50000]\n",
      "loss:2.005918 [49280/50000]\n",
      "loss:1.911310 [49920/50000]\n",
      "test error:\n",
      " accuracy:27.9%,avg loss: 1.980015 \n",
      "\n",
      "epoch 2 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.928066 [    0/50000]\n",
      "loss:1.923734 [  640/50000]\n",
      "loss:1.968998 [ 1280/50000]\n",
      "loss:1.994754 [ 1920/50000]\n",
      "loss:1.905978 [ 2560/50000]\n",
      "loss:1.928613 [ 3200/50000]\n",
      "loss:2.118163 [ 3840/50000]\n",
      "loss:1.993760 [ 4480/50000]\n",
      "loss:1.876338 [ 5120/50000]\n",
      "loss:1.953399 [ 5760/50000]\n",
      "loss:1.955857 [ 6400/50000]\n",
      "loss:2.049606 [ 7040/50000]\n",
      "loss:1.888921 [ 7680/50000]\n",
      "loss:1.944320 [ 8320/50000]\n",
      "loss:2.110921 [ 8960/50000]\n",
      "loss:1.918118 [ 9600/50000]\n",
      "loss:1.838194 [10240/50000]\n",
      "loss:1.848830 [10880/50000]\n",
      "loss:1.951992 [11520/50000]\n",
      "loss:1.834871 [12160/50000]\n",
      "loss:1.966385 [12800/50000]\n",
      "loss:1.943685 [13440/50000]\n",
      "loss:1.973931 [14080/50000]\n",
      "loss:2.036429 [14720/50000]\n",
      "loss:1.866313 [15360/50000]\n",
      "loss:1.982731 [16000/50000]\n",
      "loss:1.939509 [16640/50000]\n",
      "loss:1.931586 [17280/50000]\n",
      "loss:1.938668 [17920/50000]\n",
      "loss:1.988920 [18560/50000]\n",
      "loss:1.901952 [19200/50000]\n",
      "loss:1.970088 [19840/50000]\n",
      "loss:1.948444 [20480/50000]\n",
      "loss:1.865845 [21120/50000]\n",
      "loss:1.628288 [21760/50000]\n",
      "loss:1.991034 [22400/50000]\n",
      "loss:1.811126 [23040/50000]\n",
      "loss:1.992106 [23680/50000]\n",
      "loss:1.908760 [24320/50000]\n",
      "loss:2.059105 [24960/50000]\n",
      "loss:1.857476 [25600/50000]\n",
      "loss:1.885825 [26240/50000]\n",
      "loss:2.038880 [26880/50000]\n",
      "loss:1.957093 [27520/50000]\n",
      "loss:1.940650 [28160/50000]\n",
      "loss:1.947027 [28800/50000]\n",
      "loss:1.786742 [29440/50000]\n",
      "loss:1.772087 [30080/50000]\n",
      "loss:1.894278 [30720/50000]\n",
      "loss:1.940815 [31360/50000]\n",
      "loss:1.976462 [32000/50000]\n",
      "loss:1.673502 [32640/50000]\n",
      "loss:1.935945 [33280/50000]\n",
      "loss:1.951727 [33920/50000]\n",
      "loss:1.909380 [34560/50000]\n",
      "loss:1.740993 [35200/50000]\n",
      "loss:1.895616 [35840/50000]\n",
      "loss:1.952918 [36480/50000]\n",
      "loss:1.855044 [37120/50000]\n",
      "loss:1.778691 [37760/50000]\n",
      "loss:1.770895 [38400/50000]\n",
      "loss:1.896636 [39040/50000]\n",
      "loss:2.040418 [39680/50000]\n",
      "loss:2.099603 [40320/50000]\n",
      "loss:1.841388 [40960/50000]\n",
      "loss:2.024716 [41600/50000]\n",
      "loss:1.978112 [42240/50000]\n",
      "loss:1.835719 [42880/50000]\n",
      "loss:2.009085 [43520/50000]\n",
      "loss:1.854916 [44160/50000]\n",
      "loss:1.926480 [44800/50000]\n",
      "loss:1.925534 [45440/50000]\n",
      "loss:1.869272 [46080/50000]\n",
      "loss:1.786419 [46720/50000]\n",
      "loss:1.814333 [47360/50000]\n",
      "loss:1.992394 [48000/50000]\n",
      "loss:1.833555 [48640/50000]\n",
      "loss:1.871921 [49280/50000]\n",
      "loss:2.003981 [49920/50000]\n",
      "test error:\n",
      " accuracy:32.7%,avg loss: 1.880223 \n",
      "\n",
      "epoch 3 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.765511 [    0/50000]\n",
      "loss:1.800929 [  640/50000]\n",
      "loss:1.860740 [ 1280/50000]\n",
      "loss:2.155304 [ 1920/50000]\n",
      "loss:1.857714 [ 2560/50000]\n",
      "loss:1.807886 [ 3200/50000]\n",
      "loss:1.853136 [ 3840/50000]\n",
      "loss:1.925451 [ 4480/50000]\n",
      "loss:1.892267 [ 5120/50000]\n",
      "loss:2.104569 [ 5760/50000]\n",
      "loss:1.699808 [ 6400/50000]\n",
      "loss:1.891669 [ 7040/50000]\n",
      "loss:2.161733 [ 7680/50000]\n",
      "loss:1.782413 [ 8320/50000]\n",
      "loss:1.951340 [ 8960/50000]\n",
      "loss:1.953310 [ 9600/50000]\n",
      "loss:1.863447 [10240/50000]\n",
      "loss:1.765697 [10880/50000]\n",
      "loss:1.752481 [11520/50000]\n",
      "loss:1.834389 [12160/50000]\n",
      "loss:1.671010 [12800/50000]\n",
      "loss:1.828259 [13440/50000]\n",
      "loss:1.766982 [14080/50000]\n",
      "loss:1.966693 [14720/50000]\n",
      "loss:1.825760 [15360/50000]\n",
      "loss:1.653082 [16000/50000]\n",
      "loss:1.687245 [16640/50000]\n",
      "loss:1.664035 [17280/50000]\n",
      "loss:2.036768 [17920/50000]\n",
      "loss:1.772368 [18560/50000]\n",
      "loss:1.811123 [19200/50000]\n",
      "loss:1.857211 [19840/50000]\n",
      "loss:1.696303 [20480/50000]\n",
      "loss:1.849709 [21120/50000]\n",
      "loss:1.570756 [21760/50000]\n",
      "loss:1.646824 [22400/50000]\n",
      "loss:1.889949 [23040/50000]\n",
      "loss:1.712653 [23680/50000]\n",
      "loss:1.900158 [24320/50000]\n",
      "loss:1.834373 [24960/50000]\n",
      "loss:1.793792 [25600/50000]\n",
      "loss:1.955755 [26240/50000]\n",
      "loss:1.685536 [26880/50000]\n",
      "loss:1.724499 [27520/50000]\n",
      "loss:1.865902 [28160/50000]\n",
      "loss:1.964737 [28800/50000]\n",
      "loss:1.800630 [29440/50000]\n",
      "loss:1.834374 [30080/50000]\n",
      "loss:1.851876 [30720/50000]\n",
      "loss:1.941289 [31360/50000]\n",
      "loss:1.843602 [32000/50000]\n",
      "loss:1.808878 [32640/50000]\n",
      "loss:1.868052 [33280/50000]\n",
      "loss:1.880323 [33920/50000]\n",
      "loss:1.790631 [34560/50000]\n",
      "loss:1.819914 [35200/50000]\n",
      "loss:1.669149 [35840/50000]\n",
      "loss:1.738848 [36480/50000]\n",
      "loss:1.734859 [37120/50000]\n",
      "loss:1.961358 [37760/50000]\n",
      "loss:1.631930 [38400/50000]\n",
      "loss:1.855701 [39040/50000]\n",
      "loss:1.912109 [39680/50000]\n",
      "loss:1.590144 [40320/50000]\n",
      "loss:1.883055 [40960/50000]\n",
      "loss:1.857601 [41600/50000]\n",
      "loss:1.724736 [42240/50000]\n",
      "loss:1.916536 [42880/50000]\n",
      "loss:1.806174 [43520/50000]\n",
      "loss:1.932952 [44160/50000]\n",
      "loss:1.925919 [44800/50000]\n",
      "loss:1.662277 [45440/50000]\n",
      "loss:1.824701 [46080/50000]\n",
      "loss:1.820385 [46720/50000]\n",
      "loss:1.923564 [47360/50000]\n",
      "loss:1.887896 [48000/50000]\n",
      "loss:1.621607 [48640/50000]\n",
      "loss:1.840375 [49280/50000]\n",
      "loss:1.807826 [49920/50000]\n",
      "test error:\n",
      " accuracy:32.8%,avg loss: 1.832353 \n",
      "\n",
      "epoch 4 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.667622 [    0/50000]\n",
      "loss:1.909253 [  640/50000]\n",
      "loss:1.797362 [ 1280/50000]\n",
      "loss:1.883455 [ 1920/50000]\n",
      "loss:1.654382 [ 2560/50000]\n",
      "loss:2.074504 [ 3200/50000]\n",
      "loss:1.787045 [ 3840/50000]\n",
      "loss:1.762316 [ 4480/50000]\n",
      "loss:1.652214 [ 5120/50000]\n",
      "loss:1.829599 [ 5760/50000]\n",
      "loss:1.669349 [ 6400/50000]\n",
      "loss:1.662329 [ 7040/50000]\n",
      "loss:1.710702 [ 7680/50000]\n",
      "loss:1.725921 [ 8320/50000]\n",
      "loss:1.806341 [ 8960/50000]\n",
      "loss:1.905806 [ 9600/50000]\n",
      "loss:1.766368 [10240/50000]\n",
      "loss:1.748261 [10880/50000]\n",
      "loss:1.800855 [11520/50000]\n",
      "loss:1.567361 [12160/50000]\n",
      "loss:1.602527 [12800/50000]\n",
      "loss:1.780743 [13440/50000]\n",
      "loss:1.777062 [14080/50000]\n",
      "loss:1.800790 [14720/50000]\n",
      "loss:1.663483 [15360/50000]\n",
      "loss:1.899561 [16000/50000]\n",
      "loss:1.573231 [16640/50000]\n",
      "loss:1.688756 [17280/50000]\n",
      "loss:1.716803 [17920/50000]\n",
      "loss:1.658357 [18560/50000]\n",
      "loss:1.779222 [19200/50000]\n",
      "loss:1.883306 [19840/50000]\n",
      "loss:1.845302 [20480/50000]\n",
      "loss:1.935123 [21120/50000]\n",
      "loss:1.797162 [21760/50000]\n",
      "loss:1.727625 [22400/50000]\n",
      "loss:1.728868 [23040/50000]\n",
      "loss:1.807089 [23680/50000]\n",
      "loss:1.733783 [24320/50000]\n",
      "loss:1.699667 [24960/50000]\n",
      "loss:1.767020 [25600/50000]\n",
      "loss:1.804080 [26240/50000]\n",
      "loss:1.636814 [26880/50000]\n",
      "loss:1.779806 [27520/50000]\n",
      "loss:1.832704 [28160/50000]\n",
      "loss:1.914204 [28800/50000]\n",
      "loss:1.833463 [29440/50000]\n",
      "loss:1.744377 [30080/50000]\n",
      "loss:1.625574 [30720/50000]\n",
      "loss:1.871011 [31360/50000]\n",
      "loss:1.769385 [32000/50000]\n",
      "loss:1.748985 [32640/50000]\n",
      "loss:1.822227 [33280/50000]\n",
      "loss:1.727734 [33920/50000]\n",
      "loss:1.924581 [34560/50000]\n",
      "loss:1.889879 [35200/50000]\n",
      "loss:1.762533 [35840/50000]\n",
      "loss:1.831827 [36480/50000]\n",
      "loss:1.601204 [37120/50000]\n",
      "loss:1.808734 [37760/50000]\n",
      "loss:1.787266 [38400/50000]\n",
      "loss:1.767847 [39040/50000]\n",
      "loss:1.767277 [39680/50000]\n",
      "loss:1.634875 [40320/50000]\n",
      "loss:1.726858 [40960/50000]\n",
      "loss:1.690683 [41600/50000]\n",
      "loss:1.620996 [42240/50000]\n",
      "loss:1.817171 [42880/50000]\n",
      "loss:1.727029 [43520/50000]\n",
      "loss:1.480464 [44160/50000]\n",
      "loss:1.798077 [44800/50000]\n",
      "loss:1.928884 [45440/50000]\n",
      "loss:1.934987 [46080/50000]\n",
      "loss:1.886917 [46720/50000]\n",
      "loss:1.725618 [47360/50000]\n",
      "loss:1.703714 [48000/50000]\n",
      "loss:1.776456 [48640/50000]\n",
      "loss:1.762539 [49280/50000]\n",
      "loss:1.736182 [49920/50000]\n",
      "test error:\n",
      " accuracy:35.8%,avg loss: 1.787117 \n",
      "\n",
      "epoch 5 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.754619 [    0/50000]\n",
      "loss:2.004671 [  640/50000]\n",
      "loss:1.641752 [ 1280/50000]\n",
      "loss:1.710701 [ 1920/50000]\n",
      "loss:1.731180 [ 2560/50000]\n",
      "loss:1.671104 [ 3200/50000]\n",
      "loss:1.902420 [ 3840/50000]\n",
      "loss:1.702761 [ 4480/50000]\n",
      "loss:1.672452 [ 5120/50000]\n",
      "loss:1.643236 [ 5760/50000]\n",
      "loss:1.590081 [ 6400/50000]\n",
      "loss:1.724435 [ 7040/50000]\n",
      "loss:1.824010 [ 7680/50000]\n",
      "loss:1.681925 [ 8320/50000]\n",
      "loss:1.704143 [ 8960/50000]\n",
      "loss:1.741192 [ 9600/50000]\n",
      "loss:1.617278 [10240/50000]\n",
      "loss:1.552810 [10880/50000]\n",
      "loss:1.768045 [11520/50000]\n",
      "loss:1.811181 [12160/50000]\n",
      "loss:1.593780 [12800/50000]\n",
      "loss:1.966138 [13440/50000]\n",
      "loss:1.822788 [14080/50000]\n",
      "loss:1.586205 [14720/50000]\n",
      "loss:2.006971 [15360/50000]\n",
      "loss:1.743687 [16000/50000]\n",
      "loss:1.836581 [16640/50000]\n",
      "loss:1.601506 [17280/50000]\n",
      "loss:1.622685 [17920/50000]\n",
      "loss:1.516609 [18560/50000]\n",
      "loss:1.548187 [19200/50000]\n",
      "loss:1.753309 [19840/50000]\n",
      "loss:1.794677 [20480/50000]\n",
      "loss:1.630305 [21120/50000]\n",
      "loss:1.761791 [21760/50000]\n",
      "loss:1.640136 [22400/50000]\n",
      "loss:1.654420 [23040/50000]\n",
      "loss:1.725139 [23680/50000]\n",
      "loss:1.905478 [24320/50000]\n",
      "loss:1.454356 [24960/50000]\n",
      "loss:1.706021 [25600/50000]\n",
      "loss:1.636832 [26240/50000]\n",
      "loss:1.816096 [26880/50000]\n",
      "loss:1.550259 [27520/50000]\n",
      "loss:1.641572 [28160/50000]\n",
      "loss:1.817896 [28800/50000]\n",
      "loss:1.844322 [29440/50000]\n",
      "loss:1.578312 [30080/50000]\n",
      "loss:1.720210 [30720/50000]\n",
      "loss:1.652511 [31360/50000]\n",
      "loss:1.676434 [32000/50000]\n",
      "loss:1.647227 [32640/50000]\n",
      "loss:1.742167 [33280/50000]\n",
      "loss:1.702033 [33920/50000]\n",
      "loss:1.757677 [34560/50000]\n",
      "loss:1.783797 [35200/50000]\n",
      "loss:1.830779 [35840/50000]\n",
      "loss:1.603735 [36480/50000]\n",
      "loss:1.648398 [37120/50000]\n",
      "loss:1.651607 [37760/50000]\n",
      "loss:1.626498 [38400/50000]\n",
      "loss:1.527006 [39040/50000]\n",
      "loss:1.685305 [39680/50000]\n",
      "loss:1.699143 [40320/50000]\n",
      "loss:1.421256 [40960/50000]\n",
      "loss:1.633542 [41600/50000]\n",
      "loss:1.806975 [42240/50000]\n",
      "loss:1.810951 [42880/50000]\n",
      "loss:1.711224 [43520/50000]\n",
      "loss:1.588962 [44160/50000]\n",
      "loss:1.617450 [44800/50000]\n",
      "loss:1.705927 [45440/50000]\n",
      "loss:1.600666 [46080/50000]\n",
      "loss:1.747014 [46720/50000]\n",
      "loss:1.659738 [47360/50000]\n",
      "loss:1.537350 [48000/50000]\n",
      "loss:1.852262 [48640/50000]\n",
      "loss:1.753425 [49280/50000]\n",
      "loss:1.593633 [49920/50000]\n",
      "test error:\n",
      " accuracy:36.0%,avg loss: 1.760142 \n",
      "\n",
      "epoch 6 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.719363 [    0/50000]\n",
      "loss:1.624807 [  640/50000]\n",
      "loss:1.680885 [ 1280/50000]\n",
      "loss:1.591580 [ 1920/50000]\n",
      "loss:1.665842 [ 2560/50000]\n",
      "loss:1.727954 [ 3200/50000]\n",
      "loss:1.636324 [ 3840/50000]\n",
      "loss:1.974416 [ 4480/50000]\n",
      "loss:1.737072 [ 5120/50000]\n",
      "loss:1.685123 [ 5760/50000]\n",
      "loss:1.647747 [ 6400/50000]\n",
      "loss:1.577282 [ 7040/50000]\n",
      "loss:1.606375 [ 7680/50000]\n",
      "loss:1.687108 [ 8320/50000]\n",
      "loss:1.680417 [ 8960/50000]\n",
      "loss:1.687894 [ 9600/50000]\n",
      "loss:1.752751 [10240/50000]\n",
      "loss:1.690512 [10880/50000]\n",
      "loss:1.740211 [11520/50000]\n",
      "loss:1.767131 [12160/50000]\n",
      "loss:1.885746 [12800/50000]\n",
      "loss:1.696488 [13440/50000]\n",
      "loss:1.619165 [14080/50000]\n",
      "loss:1.764939 [14720/50000]\n",
      "loss:1.712916 [15360/50000]\n",
      "loss:1.609202 [16000/50000]\n",
      "loss:1.480633 [16640/50000]\n",
      "loss:1.818075 [17280/50000]\n",
      "loss:1.404492 [17920/50000]\n",
      "loss:1.673391 [18560/50000]\n",
      "loss:1.604364 [19200/50000]\n",
      "loss:1.523562 [19840/50000]\n",
      "loss:1.779361 [20480/50000]\n",
      "loss:1.681054 [21120/50000]\n",
      "loss:1.628589 [21760/50000]\n",
      "loss:1.650330 [22400/50000]\n",
      "loss:1.612144 [23040/50000]\n",
      "loss:1.637711 [23680/50000]\n",
      "loss:1.738885 [24320/50000]\n",
      "loss:1.726526 [24960/50000]\n",
      "loss:1.576791 [25600/50000]\n",
      "loss:1.641656 [26240/50000]\n",
      "loss:1.496312 [26880/50000]\n",
      "loss:1.787856 [27520/50000]\n",
      "loss:1.921016 [28160/50000]\n",
      "loss:1.680327 [28800/50000]\n",
      "loss:1.744271 [29440/50000]\n",
      "loss:1.879423 [30080/50000]\n",
      "loss:1.668353 [30720/50000]\n",
      "loss:1.684255 [31360/50000]\n",
      "loss:1.730917 [32000/50000]\n",
      "loss:1.700088 [32640/50000]\n",
      "loss:1.465738 [33280/50000]\n",
      "loss:1.563454 [33920/50000]\n",
      "loss:1.718313 [34560/50000]\n",
      "loss:1.723664 [35200/50000]\n",
      "loss:1.614042 [35840/50000]\n",
      "loss:1.570588 [36480/50000]\n",
      "loss:1.520993 [37120/50000]\n",
      "loss:1.870717 [37760/50000]\n",
      "loss:1.815692 [38400/50000]\n",
      "loss:1.720365 [39040/50000]\n",
      "loss:1.657528 [39680/50000]\n",
      "loss:1.564893 [40320/50000]\n",
      "loss:1.636444 [40960/50000]\n",
      "loss:1.542090 [41600/50000]\n",
      "loss:1.674472 [42240/50000]\n",
      "loss:1.835773 [42880/50000]\n",
      "loss:1.485961 [43520/50000]\n",
      "loss:1.529188 [44160/50000]\n",
      "loss:1.728919 [44800/50000]\n",
      "loss:1.894104 [45440/50000]\n",
      "loss:1.419098 [46080/50000]\n",
      "loss:1.707673 [46720/50000]\n",
      "loss:1.828522 [47360/50000]\n",
      "loss:1.633368 [48000/50000]\n",
      "loss:1.774018 [48640/50000]\n",
      "loss:1.670672 [49280/50000]\n",
      "loss:1.727078 [49920/50000]\n",
      "test error:\n",
      " accuracy:40.7%,avg loss: 1.667131 \n",
      "\n",
      "epoch 7 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.527127 [    0/50000]\n",
      "loss:1.745131 [  640/50000]\n",
      "loss:1.696276 [ 1280/50000]\n",
      "loss:1.609681 [ 1920/50000]\n",
      "loss:1.430383 [ 2560/50000]\n",
      "loss:1.557849 [ 3200/50000]\n",
      "loss:1.646098 [ 3840/50000]\n",
      "loss:1.754488 [ 4480/50000]\n",
      "loss:1.824603 [ 5120/50000]\n",
      "loss:1.741254 [ 5760/50000]\n",
      "loss:1.692822 [ 6400/50000]\n",
      "loss:1.558213 [ 7040/50000]\n",
      "loss:1.558377 [ 7680/50000]\n",
      "loss:1.682111 [ 8320/50000]\n",
      "loss:1.670615 [ 8960/50000]\n",
      "loss:1.567747 [ 9600/50000]\n",
      "loss:1.843897 [10240/50000]\n",
      "loss:1.671872 [10880/50000]\n",
      "loss:1.633709 [11520/50000]\n",
      "loss:1.534622 [12160/50000]\n",
      "loss:1.658456 [12800/50000]\n",
      "loss:1.741791 [13440/50000]\n",
      "loss:1.532503 [14080/50000]\n",
      "loss:1.466962 [14720/50000]\n",
      "loss:1.455593 [15360/50000]\n",
      "loss:1.569563 [16000/50000]\n",
      "loss:1.561386 [16640/50000]\n",
      "loss:1.635914 [17280/50000]\n",
      "loss:1.579666 [17920/50000]\n",
      "loss:1.682078 [18560/50000]\n",
      "loss:1.501026 [19200/50000]\n",
      "loss:1.688687 [19840/50000]\n",
      "loss:1.668175 [20480/50000]\n",
      "loss:1.713902 [21120/50000]\n",
      "loss:1.741476 [21760/50000]\n",
      "loss:1.664800 [22400/50000]\n",
      "loss:1.495676 [23040/50000]\n",
      "loss:1.489404 [23680/50000]\n",
      "loss:1.536665 [24320/50000]\n",
      "loss:1.620543 [24960/50000]\n",
      "loss:1.786912 [25600/50000]\n",
      "loss:1.736638 [26240/50000]\n",
      "loss:1.585096 [26880/50000]\n",
      "loss:1.943798 [27520/50000]\n",
      "loss:1.615717 [28160/50000]\n",
      "loss:1.501179 [28800/50000]\n",
      "loss:1.498638 [29440/50000]\n",
      "loss:1.722189 [30080/50000]\n",
      "loss:1.432013 [30720/50000]\n",
      "loss:1.522479 [31360/50000]\n",
      "loss:1.465515 [32000/50000]\n",
      "loss:1.478748 [32640/50000]\n",
      "loss:1.512139 [33280/50000]\n",
      "loss:1.498216 [33920/50000]\n",
      "loss:1.455234 [34560/50000]\n",
      "loss:1.482785 [35200/50000]\n",
      "loss:1.663455 [35840/50000]\n",
      "loss:1.767617 [36480/50000]\n",
      "loss:1.562689 [37120/50000]\n",
      "loss:1.814391 [37760/50000]\n",
      "loss:1.515167 [38400/50000]\n",
      "loss:1.765366 [39040/50000]\n",
      "loss:1.633206 [39680/50000]\n",
      "loss:1.769101 [40320/50000]\n",
      "loss:1.509673 [40960/50000]\n",
      "loss:1.570711 [41600/50000]\n",
      "loss:1.611512 [42240/50000]\n",
      "loss:1.682429 [42880/50000]\n",
      "loss:1.692619 [43520/50000]\n",
      "loss:1.623111 [44160/50000]\n",
      "loss:1.624331 [44800/50000]\n",
      "loss:1.514374 [45440/50000]\n",
      "loss:1.499060 [46080/50000]\n",
      "loss:1.714606 [46720/50000]\n",
      "loss:1.735060 [47360/50000]\n",
      "loss:1.588867 [48000/50000]\n",
      "loss:1.689214 [48640/50000]\n",
      "loss:1.682680 [49280/50000]\n",
      "loss:1.549878 [49920/50000]\n",
      "test error:\n",
      " accuracy:39.8%,avg loss: 1.688339 \n",
      "\n",
      "epoch 8 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.593015 [    0/50000]\n",
      "loss:1.547647 [  640/50000]\n",
      "loss:1.741987 [ 1280/50000]\n",
      "loss:1.623183 [ 1920/50000]\n",
      "loss:1.493958 [ 2560/50000]\n",
      "loss:1.443828 [ 3200/50000]\n",
      "loss:1.843340 [ 3840/50000]\n",
      "loss:1.566281 [ 4480/50000]\n",
      "loss:1.662402 [ 5120/50000]\n",
      "loss:1.862025 [ 5760/50000]\n",
      "loss:1.643160 [ 6400/50000]\n",
      "loss:1.761052 [ 7040/50000]\n",
      "loss:1.546134 [ 7680/50000]\n",
      "loss:1.686066 [ 8320/50000]\n",
      "loss:1.732574 [ 8960/50000]\n",
      "loss:1.742858 [ 9600/50000]\n",
      "loss:1.637216 [10240/50000]\n",
      "loss:1.805122 [10880/50000]\n",
      "loss:1.568523 [11520/50000]\n",
      "loss:1.642311 [12160/50000]\n",
      "loss:1.601613 [12800/50000]\n",
      "loss:1.713449 [13440/50000]\n",
      "loss:1.620885 [14080/50000]\n",
      "loss:1.659186 [14720/50000]\n",
      "loss:1.517671 [15360/50000]\n",
      "loss:1.654354 [16000/50000]\n",
      "loss:1.558512 [16640/50000]\n",
      "loss:1.567684 [17280/50000]\n",
      "loss:1.658265 [17920/50000]\n",
      "loss:1.753142 [18560/50000]\n",
      "loss:1.610272 [19200/50000]\n",
      "loss:1.784155 [19840/50000]\n",
      "loss:1.705433 [20480/50000]\n",
      "loss:1.647714 [21120/50000]\n",
      "loss:1.543143 [21760/50000]\n",
      "loss:1.515872 [22400/50000]\n",
      "loss:1.685944 [23040/50000]\n",
      "loss:1.612983 [23680/50000]\n",
      "loss:1.656264 [24320/50000]\n",
      "loss:1.526819 [24960/50000]\n",
      "loss:1.401681 [25600/50000]\n",
      "loss:1.398488 [26240/50000]\n",
      "loss:1.509967 [26880/50000]\n",
      "loss:1.528924 [27520/50000]\n",
      "loss:1.498164 [28160/50000]\n",
      "loss:1.647925 [28800/50000]\n",
      "loss:1.455204 [29440/50000]\n",
      "loss:1.529751 [30080/50000]\n",
      "loss:1.497998 [30720/50000]\n",
      "loss:1.372788 [31360/50000]\n",
      "loss:1.613151 [32000/50000]\n",
      "loss:1.597244 [32640/50000]\n",
      "loss:1.577545 [33280/50000]\n",
      "loss:1.699734 [33920/50000]\n",
      "loss:1.692966 [34560/50000]\n",
      "loss:1.575807 [35200/50000]\n",
      "loss:1.568391 [35840/50000]\n",
      "loss:1.765202 [36480/50000]\n",
      "loss:1.692748 [37120/50000]\n",
      "loss:1.415011 [37760/50000]\n",
      "loss:1.646558 [38400/50000]\n",
      "loss:1.683094 [39040/50000]\n",
      "loss:1.604857 [39680/50000]\n",
      "loss:1.608037 [40320/50000]\n",
      "loss:1.662684 [40960/50000]\n",
      "loss:1.523730 [41600/50000]\n",
      "loss:1.590312 [42240/50000]\n",
      "loss:1.634940 [42880/50000]\n",
      "loss:1.328665 [43520/50000]\n",
      "loss:1.706585 [44160/50000]\n",
      "loss:1.756521 [44800/50000]\n",
      "loss:1.638139 [45440/50000]\n",
      "loss:1.529870 [46080/50000]\n",
      "loss:1.405975 [46720/50000]\n",
      "loss:1.676418 [47360/50000]\n",
      "loss:1.733160 [48000/50000]\n",
      "loss:1.614362 [48640/50000]\n",
      "loss:1.607705 [49280/50000]\n",
      "loss:1.654823 [49920/50000]\n",
      "test error:\n",
      " accuracy:40.7%,avg loss: 1.681943 \n",
      "\n",
      "epoch 9 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.560807 [    0/50000]\n",
      "loss:1.742756 [  640/50000]\n",
      "loss:1.500890 [ 1280/50000]\n",
      "loss:1.791091 [ 1920/50000]\n",
      "loss:1.594626 [ 2560/50000]\n",
      "loss:1.589645 [ 3200/50000]\n",
      "loss:1.540828 [ 3840/50000]\n",
      "loss:1.583320 [ 4480/50000]\n",
      "loss:1.717695 [ 5120/50000]\n",
      "loss:1.466393 [ 5760/50000]\n",
      "loss:1.624145 [ 6400/50000]\n",
      "loss:1.584369 [ 7040/50000]\n",
      "loss:1.624549 [ 7680/50000]\n",
      "loss:1.643949 [ 8320/50000]\n",
      "loss:1.504797 [ 8960/50000]\n",
      "loss:1.643293 [ 9600/50000]\n",
      "loss:1.438271 [10240/50000]\n",
      "loss:1.423383 [10880/50000]\n",
      "loss:1.607988 [11520/50000]\n",
      "loss:1.458071 [12160/50000]\n",
      "loss:1.594998 [12800/50000]\n",
      "loss:1.605434 [13440/50000]\n",
      "loss:1.371033 [14080/50000]\n",
      "loss:1.659521 [14720/50000]\n",
      "loss:1.587850 [15360/50000]\n",
      "loss:1.449081 [16000/50000]\n",
      "loss:1.704981 [16640/50000]\n",
      "loss:1.632262 [17280/50000]\n",
      "loss:1.541675 [17920/50000]\n",
      "loss:1.573483 [18560/50000]\n",
      "loss:1.709986 [19200/50000]\n",
      "loss:1.530177 [19840/50000]\n",
      "loss:1.254749 [20480/50000]\n",
      "loss:1.293875 [21120/50000]\n",
      "loss:1.479377 [21760/50000]\n",
      "loss:1.441423 [22400/50000]\n",
      "loss:1.353096 [23040/50000]\n",
      "loss:1.464725 [23680/50000]\n",
      "loss:1.816070 [24320/50000]\n",
      "loss:1.784019 [24960/50000]\n",
      "loss:1.652392 [25600/50000]\n",
      "loss:1.500431 [26240/50000]\n",
      "loss:1.589459 [26880/50000]\n",
      "loss:1.662410 [27520/50000]\n",
      "loss:1.206827 [28160/50000]\n",
      "loss:1.607646 [28800/50000]\n",
      "loss:1.664217 [29440/50000]\n",
      "loss:1.524425 [30080/50000]\n",
      "loss:1.574204 [30720/50000]\n",
      "loss:1.551048 [31360/50000]\n",
      "loss:1.421332 [32000/50000]\n",
      "loss:1.712758 [32640/50000]\n",
      "loss:1.419636 [33280/50000]\n",
      "loss:1.538651 [33920/50000]\n",
      "loss:1.464641 [34560/50000]\n",
      "loss:1.670246 [35200/50000]\n",
      "loss:1.482553 [35840/50000]\n",
      "loss:1.532307 [36480/50000]\n",
      "loss:1.840893 [37120/50000]\n",
      "loss:1.508623 [37760/50000]\n",
      "loss:1.434849 [38400/50000]\n",
      "loss:1.346083 [39040/50000]\n",
      "loss:1.587315 [39680/50000]\n",
      "loss:1.570776 [40320/50000]\n",
      "loss:1.514458 [40960/50000]\n",
      "loss:1.746657 [41600/50000]\n",
      "loss:1.681163 [42240/50000]\n",
      "loss:1.539528 [42880/50000]\n",
      "loss:1.506442 [43520/50000]\n",
      "loss:1.679098 [44160/50000]\n",
      "loss:1.578836 [44800/50000]\n",
      "loss:1.508039 [45440/50000]\n",
      "loss:1.764871 [46080/50000]\n",
      "loss:1.775920 [46720/50000]\n",
      "loss:1.470063 [47360/50000]\n",
      "loss:1.693414 [48000/50000]\n",
      "loss:1.549878 [48640/50000]\n",
      "loss:1.411671 [49280/50000]\n",
      "loss:1.744284 [49920/50000]\n",
      "test error:\n",
      " accuracy:42.5%,avg loss: 1.604857 \n",
      "\n",
      "epoch 10 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.518353 [    0/50000]\n",
      "loss:1.622492 [  640/50000]\n",
      "loss:1.703914 [ 1280/50000]\n",
      "loss:1.420629 [ 1920/50000]\n",
      "loss:1.563962 [ 2560/50000]\n",
      "loss:1.538962 [ 3200/50000]\n",
      "loss:1.675434 [ 3840/50000]\n",
      "loss:1.619565 [ 4480/50000]\n",
      "loss:1.462907 [ 5120/50000]\n",
      "loss:1.579882 [ 5760/50000]\n",
      "loss:1.483840 [ 6400/50000]\n",
      "loss:1.592553 [ 7040/50000]\n",
      "loss:1.413929 [ 7680/50000]\n",
      "loss:1.559071 [ 8320/50000]\n",
      "loss:1.608687 [ 8960/50000]\n",
      "loss:1.516225 [ 9600/50000]\n",
      "loss:1.436487 [10240/50000]\n",
      "loss:1.582522 [10880/50000]\n",
      "loss:1.511600 [11520/50000]\n",
      "loss:1.494224 [12160/50000]\n",
      "loss:1.869465 [12800/50000]\n",
      "loss:1.455845 [13440/50000]\n",
      "loss:1.780320 [14080/50000]\n",
      "loss:1.504846 [14720/50000]\n",
      "loss:1.553164 [15360/50000]\n",
      "loss:1.501274 [16000/50000]\n",
      "loss:1.550599 [16640/50000]\n",
      "loss:1.412170 [17280/50000]\n",
      "loss:1.618046 [17920/50000]\n",
      "loss:1.565810 [18560/50000]\n",
      "loss:1.489665 [19200/50000]\n",
      "loss:1.478304 [19840/50000]\n",
      "loss:1.455379 [20480/50000]\n",
      "loss:1.414138 [21120/50000]\n",
      "loss:1.771542 [21760/50000]\n",
      "loss:1.591470 [22400/50000]\n",
      "loss:1.496304 [23040/50000]\n",
      "loss:1.940633 [23680/50000]\n",
      "loss:1.484700 [24320/50000]\n",
      "loss:1.729149 [24960/50000]\n",
      "loss:1.676009 [25600/50000]\n",
      "loss:1.490756 [26240/50000]\n",
      "loss:1.446815 [26880/50000]\n",
      "loss:1.454383 [27520/50000]\n",
      "loss:1.695587 [28160/50000]\n",
      "loss:1.439517 [28800/50000]\n",
      "loss:1.513439 [29440/50000]\n",
      "loss:1.521411 [30080/50000]\n",
      "loss:1.646972 [30720/50000]\n",
      "loss:1.506337 [31360/50000]\n",
      "loss:1.451817 [32000/50000]\n",
      "loss:1.495654 [32640/50000]\n",
      "loss:1.504228 [33280/50000]\n",
      "loss:1.385035 [33920/50000]\n",
      "loss:1.538782 [34560/50000]\n",
      "loss:1.468856 [35200/50000]\n",
      "loss:1.437265 [35840/50000]\n",
      "loss:1.417817 [36480/50000]\n",
      "loss:1.595684 [37120/50000]\n",
      "loss:1.524730 [37760/50000]\n",
      "loss:1.641189 [38400/50000]\n",
      "loss:1.599827 [39040/50000]\n",
      "loss:1.549446 [39680/50000]\n",
      "loss:1.691552 [40320/50000]\n",
      "loss:1.530802 [40960/50000]\n",
      "loss:1.676754 [41600/50000]\n",
      "loss:1.493684 [42240/50000]\n",
      "loss:1.487639 [42880/50000]\n",
      "loss:1.605551 [43520/50000]\n",
      "loss:1.621145 [44160/50000]\n",
      "loss:1.757112 [44800/50000]\n",
      "loss:1.474464 [45440/50000]\n",
      "loss:1.444293 [46080/50000]\n",
      "loss:1.442851 [46720/50000]\n",
      "loss:1.683851 [47360/50000]\n",
      "loss:1.474694 [48000/50000]\n",
      "loss:1.485214 [48640/50000]\n",
      "loss:1.665705 [49280/50000]\n",
      "loss:1.677649 [49920/50000]\n",
      "test error:\n",
      " accuracy:43.0%,avg loss: 1.604500 \n",
      "\n",
      "epoch 11 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.481023 [    0/50000]\n",
      "loss:1.608620 [  640/50000]\n",
      "loss:1.551596 [ 1280/50000]\n",
      "loss:1.686255 [ 1920/50000]\n",
      "loss:1.786597 [ 2560/50000]\n",
      "loss:1.449965 [ 3200/50000]\n",
      "loss:1.566467 [ 3840/50000]\n",
      "loss:1.794949 [ 4480/50000]\n",
      "loss:1.689153 [ 5120/50000]\n",
      "loss:1.656484 [ 5760/50000]\n",
      "loss:1.476280 [ 6400/50000]\n",
      "loss:1.589560 [ 7040/50000]\n",
      "loss:1.462263 [ 7680/50000]\n",
      "loss:1.737183 [ 8320/50000]\n",
      "loss:1.656860 [ 8960/50000]\n",
      "loss:1.419109 [ 9600/50000]\n",
      "loss:1.663789 [10240/50000]\n",
      "loss:1.608394 [10880/50000]\n",
      "loss:1.560943 [11520/50000]\n",
      "loss:1.586278 [12160/50000]\n",
      "loss:1.431961 [12800/50000]\n",
      "loss:1.436081 [13440/50000]\n",
      "loss:1.517192 [14080/50000]\n",
      "loss:1.501527 [14720/50000]\n",
      "loss:1.308857 [15360/50000]\n",
      "loss:1.566286 [16000/50000]\n",
      "loss:1.381024 [16640/50000]\n",
      "loss:1.638397 [17280/50000]\n",
      "loss:1.295074 [17920/50000]\n",
      "loss:1.498261 [18560/50000]\n",
      "loss:1.330689 [19200/50000]\n",
      "loss:1.600133 [19840/50000]\n",
      "loss:1.396937 [20480/50000]\n",
      "loss:1.644912 [21120/50000]\n",
      "loss:1.766085 [21760/50000]\n",
      "loss:1.341735 [22400/50000]\n",
      "loss:1.658264 [23040/50000]\n",
      "loss:1.264330 [23680/50000]\n",
      "loss:1.378574 [24320/50000]\n",
      "loss:1.611916 [24960/50000]\n",
      "loss:1.500574 [25600/50000]\n",
      "loss:1.505336 [26240/50000]\n",
      "loss:1.472484 [26880/50000]\n",
      "loss:1.617197 [27520/50000]\n",
      "loss:1.449022 [28160/50000]\n",
      "loss:1.535717 [28800/50000]\n",
      "loss:1.524526 [29440/50000]\n",
      "loss:1.611485 [30080/50000]\n",
      "loss:1.439981 [30720/50000]\n",
      "loss:1.550120 [31360/50000]\n",
      "loss:1.698858 [32000/50000]\n",
      "loss:1.533997 [32640/50000]\n",
      "loss:1.568403 [33280/50000]\n",
      "loss:1.613785 [33920/50000]\n",
      "loss:1.413564 [34560/50000]\n",
      "loss:1.439391 [35200/50000]\n",
      "loss:1.572327 [35840/50000]\n",
      "loss:1.327469 [36480/50000]\n",
      "loss:1.619860 [37120/50000]\n",
      "loss:1.450605 [37760/50000]\n",
      "loss:1.617167 [38400/50000]\n",
      "loss:1.644665 [39040/50000]\n",
      "loss:1.468867 [39680/50000]\n",
      "loss:1.391307 [40320/50000]\n",
      "loss:1.660552 [40960/50000]\n",
      "loss:1.563885 [41600/50000]\n",
      "loss:1.478446 [42240/50000]\n",
      "loss:1.659839 [42880/50000]\n",
      "loss:1.647573 [43520/50000]\n",
      "loss:1.529942 [44160/50000]\n",
      "loss:2.079563 [44800/50000]\n",
      "loss:1.295582 [45440/50000]\n",
      "loss:1.637794 [46080/50000]\n",
      "loss:1.610972 [46720/50000]\n",
      "loss:1.470038 [47360/50000]\n",
      "loss:1.512396 [48000/50000]\n",
      "loss:1.592264 [48640/50000]\n",
      "loss:1.813813 [49280/50000]\n",
      "loss:1.451433 [49920/50000]\n",
      "test error:\n",
      " accuracy:43.0%,avg loss: 1.569936 \n",
      "\n",
      "epoch 12 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.453601 [    0/50000]\n",
      "loss:1.576425 [  640/50000]\n",
      "loss:1.617712 [ 1280/50000]\n",
      "loss:1.489562 [ 1920/50000]\n",
      "loss:1.563718 [ 2560/50000]\n",
      "loss:1.463605 [ 3200/50000]\n",
      "loss:1.709531 [ 3840/50000]\n",
      "loss:1.519499 [ 4480/50000]\n",
      "loss:1.533251 [ 5120/50000]\n",
      "loss:1.444713 [ 5760/50000]\n",
      "loss:1.729840 [ 6400/50000]\n",
      "loss:1.411006 [ 7040/50000]\n",
      "loss:1.431277 [ 7680/50000]\n",
      "loss:1.425440 [ 8320/50000]\n",
      "loss:1.433236 [ 8960/50000]\n",
      "loss:1.657166 [ 9600/50000]\n",
      "loss:1.656665 [10240/50000]\n",
      "loss:1.557433 [10880/50000]\n",
      "loss:1.639050 [11520/50000]\n",
      "loss:1.520199 [12160/50000]\n",
      "loss:1.461283 [12800/50000]\n",
      "loss:1.450560 [13440/50000]\n",
      "loss:1.466670 [14080/50000]\n",
      "loss:1.621428 [14720/50000]\n",
      "loss:1.350325 [15360/50000]\n",
      "loss:1.529721 [16000/50000]\n",
      "loss:1.497622 [16640/50000]\n",
      "loss:1.654205 [17280/50000]\n",
      "loss:1.835909 [17920/50000]\n",
      "loss:1.460289 [18560/50000]\n",
      "loss:1.329775 [19200/50000]\n",
      "loss:1.841038 [19840/50000]\n",
      "loss:1.434214 [20480/50000]\n",
      "loss:1.538519 [21120/50000]\n",
      "loss:1.592490 [21760/50000]\n",
      "loss:1.504646 [22400/50000]\n",
      "loss:1.287762 [23040/50000]\n",
      "loss:1.388470 [23680/50000]\n",
      "loss:1.482478 [24320/50000]\n",
      "loss:1.572750 [24960/50000]\n",
      "loss:1.446836 [25600/50000]\n",
      "loss:1.477414 [26240/50000]\n",
      "loss:1.378886 [26880/50000]\n",
      "loss:1.480925 [27520/50000]\n",
      "loss:1.390901 [28160/50000]\n",
      "loss:1.595470 [28800/50000]\n",
      "loss:1.662971 [29440/50000]\n",
      "loss:1.538886 [30080/50000]\n",
      "loss:1.530110 [30720/50000]\n",
      "loss:1.571578 [31360/50000]\n",
      "loss:1.646264 [32000/50000]\n",
      "loss:1.524849 [32640/50000]\n",
      "loss:1.488315 [33280/50000]\n",
      "loss:1.572996 [33920/50000]\n",
      "loss:1.408640 [34560/50000]\n",
      "loss:1.546917 [35200/50000]\n",
      "loss:1.623370 [35840/50000]\n",
      "loss:1.582386 [36480/50000]\n",
      "loss:1.463217 [37120/50000]\n",
      "loss:1.506402 [37760/50000]\n",
      "loss:1.390024 [38400/50000]\n",
      "loss:1.593647 [39040/50000]\n",
      "loss:1.643459 [39680/50000]\n",
      "loss:1.503646 [40320/50000]\n",
      "loss:1.649816 [40960/50000]\n",
      "loss:1.413893 [41600/50000]\n",
      "loss:1.619283 [42240/50000]\n",
      "loss:1.683858 [42880/50000]\n",
      "loss:1.445926 [43520/50000]\n",
      "loss:1.490227 [44160/50000]\n",
      "loss:1.269073 [44800/50000]\n",
      "loss:1.450947 [45440/50000]\n",
      "loss:1.345020 [46080/50000]\n",
      "loss:1.777227 [46720/50000]\n",
      "loss:1.602824 [47360/50000]\n",
      "loss:1.619835 [48000/50000]\n",
      "loss:1.452646 [48640/50000]\n",
      "loss:1.471710 [49280/50000]\n",
      "loss:1.512115 [49920/50000]\n",
      "test error:\n",
      " accuracy:39.1%,avg loss: 1.729840 \n",
      "\n",
      "epoch 13 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.708508 [    0/50000]\n",
      "loss:1.583088 [  640/50000]\n",
      "loss:1.482766 [ 1280/50000]\n",
      "loss:1.388210 [ 1920/50000]\n",
      "loss:1.417834 [ 2560/50000]\n",
      "loss:1.592044 [ 3200/50000]\n",
      "loss:1.727686 [ 3840/50000]\n",
      "loss:1.528027 [ 4480/50000]\n",
      "loss:1.402978 [ 5120/50000]\n",
      "loss:1.176511 [ 5760/50000]\n",
      "loss:1.506831 [ 6400/50000]\n",
      "loss:1.503300 [ 7040/50000]\n",
      "loss:1.458038 [ 7680/50000]\n",
      "loss:1.478626 [ 8320/50000]\n",
      "loss:1.387896 [ 8960/50000]\n",
      "loss:1.433310 [ 9600/50000]\n",
      "loss:1.391579 [10240/50000]\n",
      "loss:1.561390 [10880/50000]\n",
      "loss:1.520176 [11520/50000]\n",
      "loss:1.387191 [12160/50000]\n",
      "loss:1.461976 [12800/50000]\n",
      "loss:1.269710 [13440/50000]\n",
      "loss:1.302785 [14080/50000]\n",
      "loss:1.480232 [14720/50000]\n",
      "loss:1.500728 [15360/50000]\n",
      "loss:1.345634 [16000/50000]\n",
      "loss:1.555785 [16640/50000]\n",
      "loss:1.626485 [17280/50000]\n",
      "loss:1.336214 [17920/50000]\n",
      "loss:1.402119 [18560/50000]\n",
      "loss:1.372639 [19200/50000]\n",
      "loss:1.375348 [19840/50000]\n",
      "loss:1.496138 [20480/50000]\n",
      "loss:1.340031 [21120/50000]\n",
      "loss:1.536700 [21760/50000]\n",
      "loss:1.421380 [22400/50000]\n",
      "loss:1.378324 [23040/50000]\n",
      "loss:1.653318 [23680/50000]\n",
      "loss:1.505819 [24320/50000]\n",
      "loss:1.404079 [24960/50000]\n",
      "loss:1.338183 [25600/50000]\n",
      "loss:1.656293 [26240/50000]\n",
      "loss:1.650061 [26880/50000]\n",
      "loss:1.394506 [27520/50000]\n",
      "loss:1.493347 [28160/50000]\n",
      "loss:1.475308 [28800/50000]\n",
      "loss:1.533897 [29440/50000]\n",
      "loss:1.362775 [30080/50000]\n",
      "loss:1.494468 [30720/50000]\n",
      "loss:1.495146 [31360/50000]\n",
      "loss:1.422396 [32000/50000]\n",
      "loss:1.657614 [32640/50000]\n",
      "loss:1.470729 [33280/50000]\n",
      "loss:1.369026 [33920/50000]\n",
      "loss:1.238074 [34560/50000]\n",
      "loss:1.501433 [35200/50000]\n",
      "loss:1.391256 [35840/50000]\n",
      "loss:1.597258 [36480/50000]\n",
      "loss:1.650861 [37120/50000]\n",
      "loss:1.533517 [37760/50000]\n",
      "loss:1.648543 [38400/50000]\n",
      "loss:1.573461 [39040/50000]\n",
      "loss:1.390316 [39680/50000]\n",
      "loss:1.529176 [40320/50000]\n",
      "loss:1.493077 [40960/50000]\n",
      "loss:1.572517 [41600/50000]\n",
      "loss:1.317742 [42240/50000]\n",
      "loss:1.370191 [42880/50000]\n",
      "loss:1.613990 [43520/50000]\n",
      "loss:1.402947 [44160/50000]\n",
      "loss:1.517428 [44800/50000]\n",
      "loss:1.574864 [45440/50000]\n",
      "loss:1.441429 [46080/50000]\n",
      "loss:1.540937 [46720/50000]\n",
      "loss:1.476926 [47360/50000]\n",
      "loss:1.248735 [48000/50000]\n",
      "loss:1.388680 [48640/50000]\n",
      "loss:1.660648 [49280/50000]\n",
      "loss:1.455392 [49920/50000]\n",
      "test error:\n",
      " accuracy:46.0%,avg loss: 1.522650 \n",
      "\n",
      "epoch 14 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.465815 [    0/50000]\n",
      "loss:1.368602 [  640/50000]\n",
      "loss:1.608028 [ 1280/50000]\n",
      "loss:1.449094 [ 1920/50000]\n",
      "loss:1.382523 [ 2560/50000]\n",
      "loss:1.344655 [ 3200/50000]\n",
      "loss:1.449052 [ 3840/50000]\n",
      "loss:1.535144 [ 4480/50000]\n",
      "loss:1.556462 [ 5120/50000]\n",
      "loss:1.398270 [ 5760/50000]\n",
      "loss:1.641184 [ 6400/50000]\n",
      "loss:1.522650 [ 7040/50000]\n",
      "loss:1.264566 [ 7680/50000]\n",
      "loss:1.442561 [ 8320/50000]\n",
      "loss:1.445594 [ 8960/50000]\n",
      "loss:1.463629 [ 9600/50000]\n",
      "loss:1.493060 [10240/50000]\n",
      "loss:1.495323 [10880/50000]\n",
      "loss:1.425812 [11520/50000]\n",
      "loss:1.630454 [12160/50000]\n",
      "loss:1.582360 [12800/50000]\n",
      "loss:1.449795 [13440/50000]\n",
      "loss:1.393315 [14080/50000]\n",
      "loss:1.511797 [14720/50000]\n",
      "loss:1.447321 [15360/50000]\n",
      "loss:1.519676 [16000/50000]\n",
      "loss:1.615182 [16640/50000]\n",
      "loss:1.408478 [17280/50000]\n",
      "loss:1.390680 [17920/50000]\n",
      "loss:1.480931 [18560/50000]\n",
      "loss:1.478652 [19200/50000]\n",
      "loss:1.385395 [19840/50000]\n",
      "loss:1.357541 [20480/50000]\n",
      "loss:1.518020 [21120/50000]\n",
      "loss:1.503864 [21760/50000]\n",
      "loss:1.765590 [22400/50000]\n",
      "loss:1.621696 [23040/50000]\n",
      "loss:1.436972 [23680/50000]\n",
      "loss:1.559057 [24320/50000]\n",
      "loss:1.284398 [24960/50000]\n",
      "loss:1.303189 [25600/50000]\n",
      "loss:1.684822 [26240/50000]\n",
      "loss:1.665725 [26880/50000]\n",
      "loss:1.483778 [27520/50000]\n",
      "loss:1.659466 [28160/50000]\n",
      "loss:1.307030 [28800/50000]\n",
      "loss:1.557378 [29440/50000]\n",
      "loss:1.508773 [30080/50000]\n",
      "loss:1.379962 [30720/50000]\n",
      "loss:1.486390 [31360/50000]\n",
      "loss:1.415705 [32000/50000]\n",
      "loss:1.483263 [32640/50000]\n",
      "loss:1.265355 [33280/50000]\n",
      "loss:1.650412 [33920/50000]\n",
      "loss:1.553021 [34560/50000]\n",
      "loss:1.596999 [35200/50000]\n",
      "loss:1.534372 [35840/50000]\n",
      "loss:1.407171 [36480/50000]\n",
      "loss:1.557618 [37120/50000]\n",
      "loss:1.469637 [37760/50000]\n",
      "loss:1.243254 [38400/50000]\n",
      "loss:1.509306 [39040/50000]\n",
      "loss:1.527979 [39680/50000]\n",
      "loss:1.441247 [40320/50000]\n",
      "loss:1.233850 [40960/50000]\n",
      "loss:1.401077 [41600/50000]\n",
      "loss:1.541868 [42240/50000]\n",
      "loss:1.434200 [42880/50000]\n",
      "loss:1.411865 [43520/50000]\n",
      "loss:1.511761 [44160/50000]\n",
      "loss:1.414743 [44800/50000]\n",
      "loss:1.546892 [45440/50000]\n",
      "loss:1.359701 [46080/50000]\n",
      "loss:1.554986 [46720/50000]\n",
      "loss:1.363200 [47360/50000]\n",
      "loss:1.342321 [48000/50000]\n",
      "loss:1.462319 [48640/50000]\n",
      "loss:1.353605 [49280/50000]\n",
      "loss:1.480706 [49920/50000]\n",
      "test error:\n",
      " accuracy:44.4%,avg loss: 1.564770 \n",
      "\n",
      "epoch 15 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.460333 [    0/50000]\n",
      "loss:1.560527 [  640/50000]\n",
      "loss:1.494588 [ 1280/50000]\n",
      "loss:1.345965 [ 1920/50000]\n",
      "loss:1.532612 [ 2560/50000]\n",
      "loss:1.458728 [ 3200/50000]\n",
      "loss:1.630729 [ 3840/50000]\n",
      "loss:1.479299 [ 4480/50000]\n",
      "loss:1.504328 [ 5120/50000]\n",
      "loss:1.228087 [ 5760/50000]\n",
      "loss:1.520188 [ 6400/50000]\n",
      "loss:1.442355 [ 7040/50000]\n",
      "loss:1.227621 [ 7680/50000]\n",
      "loss:1.363433 [ 8320/50000]\n",
      "loss:1.522119 [ 8960/50000]\n",
      "loss:1.499328 [ 9600/50000]\n",
      "loss:1.504465 [10240/50000]\n",
      "loss:1.266440 [10880/50000]\n",
      "loss:1.375796 [11520/50000]\n",
      "loss:1.387139 [12160/50000]\n",
      "loss:1.615262 [12800/50000]\n",
      "loss:1.737544 [13440/50000]\n",
      "loss:1.506653 [14080/50000]\n",
      "loss:1.391814 [14720/50000]\n",
      "loss:1.187619 [15360/50000]\n",
      "loss:1.380290 [16000/50000]\n",
      "loss:1.331185 [16640/50000]\n",
      "loss:1.634771 [17280/50000]\n",
      "loss:1.711318 [17920/50000]\n",
      "loss:1.561666 [18560/50000]\n",
      "loss:1.367969 [19200/50000]\n",
      "loss:1.647623 [19840/50000]\n",
      "loss:1.531223 [20480/50000]\n",
      "loss:1.331217 [21120/50000]\n",
      "loss:1.371831 [21760/50000]\n",
      "loss:1.470322 [22400/50000]\n",
      "loss:1.237510 [23040/50000]\n",
      "loss:1.419421 [23680/50000]\n",
      "loss:1.240419 [24320/50000]\n",
      "loss:1.526179 [24960/50000]\n",
      "loss:1.479893 [25600/50000]\n",
      "loss:1.444731 [26240/50000]\n",
      "loss:1.495030 [26880/50000]\n",
      "loss:1.615963 [27520/50000]\n",
      "loss:1.650213 [28160/50000]\n",
      "loss:1.332945 [28800/50000]\n",
      "loss:1.375789 [29440/50000]\n",
      "loss:1.305127 [30080/50000]\n",
      "loss:1.134988 [30720/50000]\n",
      "loss:1.446823 [31360/50000]\n",
      "loss:1.369229 [32000/50000]\n",
      "loss:1.376406 [32640/50000]\n",
      "loss:1.448722 [33280/50000]\n",
      "loss:1.454858 [33920/50000]\n",
      "loss:1.626632 [34560/50000]\n",
      "loss:1.545889 [35200/50000]\n",
      "loss:1.427099 [35840/50000]\n",
      "loss:1.475500 [36480/50000]\n",
      "loss:1.252720 [37120/50000]\n",
      "loss:1.563133 [37760/50000]\n",
      "loss:1.480341 [38400/50000]\n",
      "loss:1.552350 [39040/50000]\n",
      "loss:1.564453 [39680/50000]\n",
      "loss:1.455204 [40320/50000]\n",
      "loss:1.250073 [40960/50000]\n",
      "loss:1.464867 [41600/50000]\n",
      "loss:1.385881 [42240/50000]\n",
      "loss:1.426754 [42880/50000]\n",
      "loss:1.366351 [43520/50000]\n",
      "loss:1.529791 [44160/50000]\n",
      "loss:1.594453 [44800/50000]\n",
      "loss:1.432261 [45440/50000]\n",
      "loss:1.440263 [46080/50000]\n",
      "loss:1.431419 [46720/50000]\n",
      "loss:1.285966 [47360/50000]\n",
      "loss:1.551024 [48000/50000]\n",
      "loss:1.514363 [48640/50000]\n",
      "loss:1.396799 [49280/50000]\n",
      "loss:1.428677 [49920/50000]\n",
      "test error:\n",
      " accuracy:42.0%,avg loss: 1.663570 \n",
      "\n",
      "epoch 16 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.722374 [    0/50000]\n",
      "loss:1.343985 [  640/50000]\n",
      "loss:1.369404 [ 1280/50000]\n",
      "loss:1.345107 [ 1920/50000]\n",
      "loss:1.451792 [ 2560/50000]\n",
      "loss:1.499938 [ 3200/50000]\n",
      "loss:1.327914 [ 3840/50000]\n",
      "loss:1.425250 [ 4480/50000]\n",
      "loss:1.827991 [ 5120/50000]\n",
      "loss:1.355587 [ 5760/50000]\n",
      "loss:1.452573 [ 6400/50000]\n",
      "loss:1.596130 [ 7040/50000]\n",
      "loss:1.183534 [ 7680/50000]\n",
      "loss:1.225569 [ 8320/50000]\n",
      "loss:1.383999 [ 8960/50000]\n",
      "loss:1.139543 [ 9600/50000]\n",
      "loss:1.692711 [10240/50000]\n",
      "loss:1.570027 [10880/50000]\n",
      "loss:1.635064 [11520/50000]\n",
      "loss:1.451968 [12160/50000]\n",
      "loss:1.254926 [12800/50000]\n",
      "loss:1.456628 [13440/50000]\n",
      "loss:1.552825 [14080/50000]\n",
      "loss:1.358241 [14720/50000]\n",
      "loss:1.478695 [15360/50000]\n",
      "loss:1.504906 [16000/50000]\n",
      "loss:1.522902 [16640/50000]\n",
      "loss:1.326968 [17280/50000]\n",
      "loss:1.758650 [17920/50000]\n",
      "loss:1.552936 [18560/50000]\n",
      "loss:1.235214 [19200/50000]\n",
      "loss:1.618480 [19840/50000]\n",
      "loss:1.381557 [20480/50000]\n",
      "loss:1.333119 [21120/50000]\n",
      "loss:1.495250 [21760/50000]\n",
      "loss:1.275023 [22400/50000]\n",
      "loss:1.475560 [23040/50000]\n",
      "loss:1.653550 [23680/50000]\n",
      "loss:1.476091 [24320/50000]\n",
      "loss:1.539878 [24960/50000]\n",
      "loss:1.597806 [25600/50000]\n",
      "loss:1.392067 [26240/50000]\n",
      "loss:1.352099 [26880/50000]\n",
      "loss:1.414793 [27520/50000]\n",
      "loss:1.418009 [28160/50000]\n",
      "loss:1.189601 [28800/50000]\n",
      "loss:1.395171 [29440/50000]\n",
      "loss:1.402363 [30080/50000]\n",
      "loss:1.378311 [30720/50000]\n",
      "loss:1.454250 [31360/50000]\n",
      "loss:1.444436 [32000/50000]\n",
      "loss:1.663591 [32640/50000]\n",
      "loss:1.383293 [33280/50000]\n",
      "loss:1.225303 [33920/50000]\n",
      "loss:1.707137 [34560/50000]\n",
      "loss:1.499335 [35200/50000]\n",
      "loss:1.624050 [35840/50000]\n",
      "loss:1.414675 [36480/50000]\n",
      "loss:1.456468 [37120/50000]\n",
      "loss:1.296796 [37760/50000]\n",
      "loss:1.481168 [38400/50000]\n",
      "loss:1.316369 [39040/50000]\n",
      "loss:1.416081 [39680/50000]\n",
      "loss:1.520807 [40320/50000]\n",
      "loss:1.440984 [40960/50000]\n",
      "loss:1.475977 [41600/50000]\n",
      "loss:1.406895 [42240/50000]\n",
      "loss:1.304733 [42880/50000]\n",
      "loss:1.471275 [43520/50000]\n",
      "loss:1.363547 [44160/50000]\n",
      "loss:1.755699 [44800/50000]\n",
      "loss:1.491464 [45440/50000]\n",
      "loss:1.506876 [46080/50000]\n",
      "loss:1.847954 [46720/50000]\n",
      "loss:1.267616 [47360/50000]\n",
      "loss:1.484858 [48000/50000]\n",
      "loss:1.569519 [48640/50000]\n",
      "loss:1.294350 [49280/50000]\n",
      "loss:1.308301 [49920/50000]\n",
      "test error:\n",
      " accuracy:44.9%,avg loss: 1.541862 \n",
      "\n",
      "epoch 17 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.517193 [    0/50000]\n",
      "loss:1.248665 [  640/50000]\n",
      "loss:1.311068 [ 1280/50000]\n",
      "loss:1.264576 [ 1920/50000]\n",
      "loss:1.386385 [ 2560/50000]\n",
      "loss:1.322429 [ 3200/50000]\n",
      "loss:1.480036 [ 3840/50000]\n",
      "loss:1.296385 [ 4480/50000]\n",
      "loss:1.479126 [ 5120/50000]\n",
      "loss:1.543215 [ 5760/50000]\n",
      "loss:1.495718 [ 6400/50000]\n",
      "loss:1.253750 [ 7040/50000]\n",
      "loss:1.357297 [ 7680/50000]\n",
      "loss:1.277289 [ 8320/50000]\n",
      "loss:1.184754 [ 8960/50000]\n",
      "loss:1.467381 [ 9600/50000]\n",
      "loss:1.551573 [10240/50000]\n",
      "loss:1.417193 [10880/50000]\n",
      "loss:1.426936 [11520/50000]\n",
      "loss:1.596363 [12160/50000]\n",
      "loss:1.421040 [12800/50000]\n",
      "loss:1.527047 [13440/50000]\n",
      "loss:1.750837 [14080/50000]\n",
      "loss:1.288832 [14720/50000]\n",
      "loss:1.252619 [15360/50000]\n",
      "loss:1.409416 [16000/50000]\n",
      "loss:1.489744 [16640/50000]\n",
      "loss:1.237214 [17280/50000]\n",
      "loss:1.772277 [17920/50000]\n",
      "loss:1.535802 [18560/50000]\n",
      "loss:1.402653 [19200/50000]\n",
      "loss:1.274966 [19840/50000]\n",
      "loss:1.389624 [20480/50000]\n",
      "loss:1.236003 [21120/50000]\n",
      "loss:1.553074 [21760/50000]\n",
      "loss:1.341055 [22400/50000]\n",
      "loss:1.311493 [23040/50000]\n",
      "loss:1.446410 [23680/50000]\n",
      "loss:1.527353 [24320/50000]\n",
      "loss:1.443519 [24960/50000]\n",
      "loss:1.477240 [25600/50000]\n",
      "loss:1.552214 [26240/50000]\n",
      "loss:1.367386 [26880/50000]\n",
      "loss:1.531929 [27520/50000]\n",
      "loss:1.534949 [28160/50000]\n",
      "loss:1.498306 [28800/50000]\n",
      "loss:1.356414 [29440/50000]\n",
      "loss:1.377542 [30080/50000]\n",
      "loss:1.362083 [30720/50000]\n",
      "loss:1.385903 [31360/50000]\n",
      "loss:1.318251 [32000/50000]\n",
      "loss:1.353416 [32640/50000]\n",
      "loss:1.476335 [33280/50000]\n",
      "loss:1.247906 [33920/50000]\n",
      "loss:1.347464 [34560/50000]\n",
      "loss:1.454505 [35200/50000]\n",
      "loss:1.471514 [35840/50000]\n",
      "loss:1.462426 [36480/50000]\n",
      "loss:1.408970 [37120/50000]\n",
      "loss:1.467955 [37760/50000]\n",
      "loss:1.479835 [38400/50000]\n",
      "loss:1.396455 [39040/50000]\n",
      "loss:1.401328 [39680/50000]\n",
      "loss:1.551162 [40320/50000]\n",
      "loss:1.298797 [40960/50000]\n",
      "loss:1.353330 [41600/50000]\n",
      "loss:1.448421 [42240/50000]\n",
      "loss:1.390618 [42880/50000]\n",
      "loss:1.290887 [43520/50000]\n",
      "loss:1.447148 [44160/50000]\n",
      "loss:1.443670 [44800/50000]\n",
      "loss:1.353570 [45440/50000]\n",
      "loss:1.284054 [46080/50000]\n",
      "loss:1.328521 [46720/50000]\n",
      "loss:1.512863 [47360/50000]\n",
      "loss:1.488641 [48000/50000]\n",
      "loss:1.384809 [48640/50000]\n",
      "loss:1.359136 [49280/50000]\n",
      "loss:1.339068 [49920/50000]\n",
      "test error:\n",
      " accuracy:46.5%,avg loss: 1.498355 \n",
      "\n",
      "epoch 18 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.548141 [    0/50000]\n",
      "loss:1.327464 [  640/50000]\n",
      "loss:1.277587 [ 1280/50000]\n",
      "loss:1.446281 [ 1920/50000]\n",
      "loss:1.482155 [ 2560/50000]\n",
      "loss:1.503104 [ 3200/50000]\n",
      "loss:1.387960 [ 3840/50000]\n",
      "loss:1.454539 [ 4480/50000]\n",
      "loss:1.581941 [ 5120/50000]\n",
      "loss:1.294286 [ 5760/50000]\n",
      "loss:1.385577 [ 6400/50000]\n",
      "loss:1.298600 [ 7040/50000]\n",
      "loss:1.334944 [ 7680/50000]\n",
      "loss:1.500763 [ 8320/50000]\n",
      "loss:1.489816 [ 8960/50000]\n",
      "loss:1.209598 [ 9600/50000]\n",
      "loss:1.390397 [10240/50000]\n",
      "loss:1.712785 [10880/50000]\n",
      "loss:1.263897 [11520/50000]\n",
      "loss:1.646009 [12160/50000]\n",
      "loss:1.300243 [12800/50000]\n",
      "loss:1.355127 [13440/50000]\n",
      "loss:1.499469 [14080/50000]\n",
      "loss:1.395735 [14720/50000]\n",
      "loss:1.531879 [15360/50000]\n",
      "loss:1.525875 [16000/50000]\n",
      "loss:1.369258 [16640/50000]\n",
      "loss:1.529637 [17280/50000]\n",
      "loss:1.502970 [17920/50000]\n",
      "loss:1.690280 [18560/50000]\n",
      "loss:1.458646 [19200/50000]\n",
      "loss:1.391175 [19840/50000]\n",
      "loss:1.551342 [20480/50000]\n",
      "loss:1.383105 [21120/50000]\n",
      "loss:1.598551 [21760/50000]\n",
      "loss:1.424574 [22400/50000]\n",
      "loss:1.384033 [23040/50000]\n",
      "loss:1.278273 [23680/50000]\n",
      "loss:1.402031 [24320/50000]\n",
      "loss:1.631111 [24960/50000]\n",
      "loss:1.329332 [25600/50000]\n",
      "loss:1.377934 [26240/50000]\n",
      "loss:1.177974 [26880/50000]\n",
      "loss:1.385050 [27520/50000]\n",
      "loss:1.413394 [28160/50000]\n",
      "loss:1.364889 [28800/50000]\n",
      "loss:1.331064 [29440/50000]\n",
      "loss:1.475664 [30080/50000]\n",
      "loss:1.267159 [30720/50000]\n",
      "loss:1.270693 [31360/50000]\n",
      "loss:1.505497 [32000/50000]\n",
      "loss:1.395316 [32640/50000]\n",
      "loss:1.331843 [33280/50000]\n",
      "loss:1.403990 [33920/50000]\n",
      "loss:1.424162 [34560/50000]\n",
      "loss:1.459121 [35200/50000]\n",
      "loss:1.379257 [35840/50000]\n",
      "loss:1.279042 [36480/50000]\n",
      "loss:1.132341 [37120/50000]\n",
      "loss:1.546132 [37760/50000]\n",
      "loss:1.219139 [38400/50000]\n",
      "loss:1.363802 [39040/50000]\n",
      "loss:1.546189 [39680/50000]\n",
      "loss:1.440195 [40320/50000]\n",
      "loss:1.544284 [40960/50000]\n",
      "loss:1.559142 [41600/50000]\n",
      "loss:1.249813 [42240/50000]\n",
      "loss:1.459451 [42880/50000]\n",
      "loss:1.421913 [43520/50000]\n",
      "loss:1.547088 [44160/50000]\n",
      "loss:1.453522 [44800/50000]\n",
      "loss:1.397861 [45440/50000]\n",
      "loss:1.195048 [46080/50000]\n",
      "loss:1.429252 [46720/50000]\n",
      "loss:1.500327 [47360/50000]\n",
      "loss:1.181194 [48000/50000]\n",
      "loss:1.361382 [48640/50000]\n",
      "loss:1.417813 [49280/50000]\n",
      "loss:1.332930 [49920/50000]\n",
      "test error:\n",
      " accuracy:44.5%,avg loss: 1.582534 \n",
      "\n",
      "epoch 19 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.559808 [    0/50000]\n",
      "loss:1.369816 [  640/50000]\n",
      "loss:1.226612 [ 1280/50000]\n",
      "loss:1.283146 [ 1920/50000]\n",
      "loss:1.297803 [ 2560/50000]\n",
      "loss:1.441475 [ 3200/50000]\n",
      "loss:1.297145 [ 3840/50000]\n",
      "loss:1.439756 [ 4480/50000]\n",
      "loss:1.332453 [ 5120/50000]\n",
      "loss:1.412398 [ 5760/50000]\n",
      "loss:1.554218 [ 6400/50000]\n",
      "loss:1.254499 [ 7040/50000]\n",
      "loss:1.321332 [ 7680/50000]\n",
      "loss:1.441378 [ 8320/50000]\n",
      "loss:1.348008 [ 8960/50000]\n",
      "loss:1.379735 [ 9600/50000]\n",
      "loss:1.512979 [10240/50000]\n",
      "loss:1.490850 [10880/50000]\n",
      "loss:1.358293 [11520/50000]\n",
      "loss:1.331595 [12160/50000]\n",
      "loss:1.331479 [12800/50000]\n",
      "loss:1.403116 [13440/50000]\n",
      "loss:1.407977 [14080/50000]\n",
      "loss:1.504144 [14720/50000]\n",
      "loss:1.397039 [15360/50000]\n",
      "loss:1.232003 [16000/50000]\n",
      "loss:1.514026 [16640/50000]\n",
      "loss:1.527740 [17280/50000]\n",
      "loss:1.415546 [17920/50000]\n",
      "loss:1.421390 [18560/50000]\n",
      "loss:1.010261 [19200/50000]\n",
      "loss:1.218422 [19840/50000]\n",
      "loss:1.616760 [20480/50000]\n",
      "loss:1.441137 [21120/50000]\n",
      "loss:1.382788 [21760/50000]\n",
      "loss:1.433504 [22400/50000]\n",
      "loss:1.219826 [23040/50000]\n",
      "loss:1.320321 [23680/50000]\n",
      "loss:1.045832 [24320/50000]\n",
      "loss:1.337102 [24960/50000]\n",
      "loss:1.470732 [25600/50000]\n",
      "loss:1.210223 [26240/50000]\n",
      "loss:1.549886 [26880/50000]\n",
      "loss:1.457563 [27520/50000]\n",
      "loss:1.456335 [28160/50000]\n",
      "loss:1.506890 [28800/50000]\n",
      "loss:1.111934 [29440/50000]\n",
      "loss:1.232120 [30080/50000]\n",
      "loss:1.309729 [30720/50000]\n",
      "loss:1.420760 [31360/50000]\n",
      "loss:1.597198 [32000/50000]\n",
      "loss:1.484827 [32640/50000]\n",
      "loss:1.294885 [33280/50000]\n",
      "loss:1.491317 [33920/50000]\n",
      "loss:1.425861 [34560/50000]\n",
      "loss:1.450227 [35200/50000]\n",
      "loss:1.368387 [35840/50000]\n",
      "loss:1.441615 [36480/50000]\n",
      "loss:1.554196 [37120/50000]\n",
      "loss:1.471672 [37760/50000]\n",
      "loss:1.449403 [38400/50000]\n",
      "loss:1.221535 [39040/50000]\n",
      "loss:1.361166 [39680/50000]\n",
      "loss:1.359042 [40320/50000]\n",
      "loss:1.180693 [40960/50000]\n",
      "loss:1.294618 [41600/50000]\n",
      "loss:1.479646 [42240/50000]\n",
      "loss:1.243352 [42880/50000]\n",
      "loss:1.464786 [43520/50000]\n",
      "loss:1.386763 [44160/50000]\n",
      "loss:1.254278 [44800/50000]\n",
      "loss:1.452745 [45440/50000]\n",
      "loss:1.380388 [46080/50000]\n",
      "loss:1.409643 [46720/50000]\n",
      "loss:1.444461 [47360/50000]\n",
      "loss:1.419768 [48000/50000]\n",
      "loss:1.519752 [48640/50000]\n",
      "loss:1.274410 [49280/50000]\n",
      "loss:1.469963 [49920/50000]\n",
      "test error:\n",
      " accuracy:48.7%,avg loss: 1.448013 \n",
      "\n",
      "epoch 20 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.173252 [    0/50000]\n",
      "loss:1.384977 [  640/50000]\n",
      "loss:1.362699 [ 1280/50000]\n",
      "loss:1.311076 [ 1920/50000]\n",
      "loss:1.469420 [ 2560/50000]\n",
      "loss:1.270626 [ 3200/50000]\n",
      "loss:1.279628 [ 3840/50000]\n",
      "loss:1.574002 [ 4480/50000]\n",
      "loss:1.400015 [ 5120/50000]\n",
      "loss:1.567873 [ 5760/50000]\n",
      "loss:1.401427 [ 6400/50000]\n",
      "loss:1.528186 [ 7040/50000]\n",
      "loss:1.299217 [ 7680/50000]\n",
      "loss:1.449200 [ 8320/50000]\n",
      "loss:1.448347 [ 8960/50000]\n",
      "loss:1.496606 [ 9600/50000]\n",
      "loss:1.314094 [10240/50000]\n",
      "loss:1.157850 [10880/50000]\n",
      "loss:1.247174 [11520/50000]\n",
      "loss:1.266223 [12160/50000]\n",
      "loss:1.396231 [12800/50000]\n",
      "loss:1.464372 [13440/50000]\n",
      "loss:1.453231 [14080/50000]\n",
      "loss:1.569032 [14720/50000]\n",
      "loss:1.561719 [15360/50000]\n",
      "loss:1.449561 [16000/50000]\n",
      "loss:1.397737 [16640/50000]\n",
      "loss:1.752623 [17280/50000]\n",
      "loss:1.465013 [17920/50000]\n",
      "loss:1.496962 [18560/50000]\n",
      "loss:1.560261 [19200/50000]\n",
      "loss:1.480180 [19840/50000]\n",
      "loss:1.431198 [20480/50000]\n",
      "loss:1.613105 [21120/50000]\n",
      "loss:1.294605 [21760/50000]\n",
      "loss:1.498162 [22400/50000]\n",
      "loss:1.568052 [23040/50000]\n",
      "loss:1.444502 [23680/50000]\n",
      "loss:1.320682 [24320/50000]\n",
      "loss:1.269664 [24960/50000]\n",
      "loss:1.353743 [25600/50000]\n",
      "loss:1.496127 [26240/50000]\n",
      "loss:1.221920 [26880/50000]\n",
      "loss:1.365837 [27520/50000]\n",
      "loss:1.214345 [28160/50000]\n",
      "loss:1.517629 [28800/50000]\n",
      "loss:1.413068 [29440/50000]\n",
      "loss:1.469306 [30080/50000]\n",
      "loss:1.429732 [30720/50000]\n",
      "loss:1.448401 [31360/50000]\n",
      "loss:1.242808 [32000/50000]\n",
      "loss:1.208562 [32640/50000]\n",
      "loss:1.448565 [33280/50000]\n",
      "loss:1.399592 [33920/50000]\n",
      "loss:1.344420 [34560/50000]\n",
      "loss:1.441666 [35200/50000]\n",
      "loss:1.330288 [35840/50000]\n",
      "loss:1.378679 [36480/50000]\n",
      "loss:1.238268 [37120/50000]\n",
      "loss:1.433670 [37760/50000]\n",
      "loss:1.333668 [38400/50000]\n",
      "loss:1.385074 [39040/50000]\n",
      "loss:1.449469 [39680/50000]\n",
      "loss:1.328771 [40320/50000]\n",
      "loss:1.484634 [40960/50000]\n",
      "loss:1.705320 [41600/50000]\n",
      "loss:1.319739 [42240/50000]\n",
      "loss:1.311207 [42880/50000]\n",
      "loss:1.295687 [43520/50000]\n",
      "loss:1.256907 [44160/50000]\n",
      "loss:1.243263 [44800/50000]\n",
      "loss:1.276393 [45440/50000]\n",
      "loss:1.414089 [46080/50000]\n",
      "loss:1.387644 [46720/50000]\n",
      "loss:1.651844 [47360/50000]\n",
      "loss:1.489981 [48000/50000]\n",
      "loss:1.326567 [48640/50000]\n",
      "loss:1.290778 [49280/50000]\n",
      "loss:1.373385 [49920/50000]\n",
      "test error:\n",
      " accuracy:44.5%,avg loss: 1.535640 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(epoch):\n",
    "    print(f\"epoch {t+1} \\n----------------------------------------------\")\n",
    "    train(trainLoader,model,loss,optimizer)\n",
    "    max_correct,correct=test(testLoader,model,loss,max_correct)\n",
    "    writer.add_scalar(\"accuracy\",correct,t)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
