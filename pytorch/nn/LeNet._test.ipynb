{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from LeNet import LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_set=torchvision.datasets.CIFAR10(root=\"../data\",train=True,transform=transform,download=False)\n",
    "test_set=torchvision.datasets.CIFAR10(root=\"../data\",train=False,transform=transform,download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img,label=train_set[0]\n",
    "# img.shape,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "lr=0.1\n",
    "epoch=20\n",
    "writer=SummaryWriter(\"logs\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trainLoader=DataLoader(train_set,batch_size=batch_size,shuffle=True,num_workers=0,drop_last=False)\n",
    "testLoader=DataLoader(test_set,batch_size=batch_size,shuffle=True,num_workers=0,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LeNet().to(device)\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "model.apply(init_weights)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainLoader,model,loss_fn,optimizer):\n",
    "    print(f\"training on:{device}\")\n",
    "    # model=model.to(device)\n",
    "    size=len(trainLoader.dataset)\n",
    "    \n",
    "    for batch,(X,y) in enumerate(trainLoader):\n",
    "        X=X.to(device)\n",
    "        y=y.to(device)\n",
    "        pred=model(X)\n",
    "        loss=loss_fn(pred,y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch%10==0:\n",
    "            loss,current=loss.item(),batch*len(X)\n",
    "            print(f\"loss:{loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_correct=0\n",
    "def test(testLoader, model, loss_fn,max_correct):\n",
    "    # model = model.to(device)\n",
    "    size = len(testLoader.dataset)\n",
    "    num_batches = len(testLoader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in testLoader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    correct /= size\n",
    "    \n",
    "    if correct>max_correct:\n",
    "        torch.save(model.state_dict(),'mlp.params')\n",
    "        max_correct=correct\n",
    "\n",
    "    print(f\"test error:\\n accuracy:{(100 * correct):>0.1f}%,avg loss: {test_loss:>8f} \\n\")\n",
    "    return max_correct,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.593022 [    0/50000]\n",
      "loss:2.306213 [  640/50000]\n",
      "loss:2.324021 [ 1280/50000]\n",
      "loss:2.323118 [ 1920/50000]\n",
      "loss:2.322331 [ 2560/50000]\n",
      "loss:2.312983 [ 3200/50000]\n",
      "loss:2.302548 [ 3840/50000]\n",
      "loss:2.292437 [ 4480/50000]\n",
      "loss:2.292272 [ 5120/50000]\n",
      "loss:2.290393 [ 5760/50000]\n",
      "loss:2.299395 [ 6400/50000]\n",
      "loss:2.296983 [ 7040/50000]\n",
      "loss:2.297990 [ 7680/50000]\n",
      "loss:2.328382 [ 8320/50000]\n",
      "loss:2.291830 [ 8960/50000]\n",
      "loss:2.293055 [ 9600/50000]\n",
      "loss:2.304286 [10240/50000]\n",
      "loss:2.309412 [10880/50000]\n",
      "loss:2.277182 [11520/50000]\n",
      "loss:2.329027 [12160/50000]\n",
      "loss:2.299432 [12800/50000]\n",
      "loss:2.285431 [13440/50000]\n",
      "loss:2.286742 [14080/50000]\n",
      "loss:2.329300 [14720/50000]\n",
      "loss:2.288618 [15360/50000]\n",
      "loss:2.311886 [16000/50000]\n",
      "loss:2.324358 [16640/50000]\n",
      "loss:2.316652 [17280/50000]\n",
      "loss:2.288066 [17920/50000]\n",
      "loss:2.299737 [18560/50000]\n",
      "loss:2.302201 [19200/50000]\n",
      "loss:2.301362 [19840/50000]\n",
      "loss:2.309028 [20480/50000]\n",
      "loss:2.319629 [21120/50000]\n",
      "loss:2.284458 [21760/50000]\n",
      "loss:2.324858 [22400/50000]\n",
      "loss:2.299978 [23040/50000]\n",
      "loss:2.301750 [23680/50000]\n",
      "loss:2.309520 [24320/50000]\n",
      "loss:2.314935 [24960/50000]\n",
      "loss:2.348811 [25600/50000]\n",
      "loss:2.327999 [26240/50000]\n",
      "loss:2.315549 [26880/50000]\n",
      "loss:2.321785 [27520/50000]\n",
      "loss:2.278371 [28160/50000]\n",
      "loss:2.285366 [28800/50000]\n",
      "loss:2.330819 [29440/50000]\n",
      "loss:2.324409 [30080/50000]\n",
      "loss:2.277122 [30720/50000]\n",
      "loss:2.330968 [31360/50000]\n",
      "loss:2.296800 [32000/50000]\n",
      "loss:2.312426 [32640/50000]\n",
      "loss:2.323632 [33280/50000]\n",
      "loss:2.332784 [33920/50000]\n",
      "loss:2.310221 [34560/50000]\n",
      "loss:2.319208 [35200/50000]\n",
      "loss:2.310699 [35840/50000]\n",
      "loss:2.315408 [36480/50000]\n",
      "loss:2.302544 [37120/50000]\n",
      "loss:2.302816 [37760/50000]\n",
      "loss:2.307392 [38400/50000]\n",
      "loss:2.285519 [39040/50000]\n",
      "loss:2.326653 [39680/50000]\n",
      "loss:2.311439 [40320/50000]\n",
      "loss:2.294878 [40960/50000]\n",
      "loss:2.326850 [41600/50000]\n",
      "loss:2.310833 [42240/50000]\n",
      "loss:2.307337 [42880/50000]\n",
      "loss:2.340322 [43520/50000]\n",
      "loss:2.301419 [44160/50000]\n",
      "loss:2.300061 [44800/50000]\n",
      "loss:2.300300 [45440/50000]\n",
      "loss:2.298413 [46080/50000]\n",
      "loss:2.290298 [46720/50000]\n",
      "loss:2.351676 [47360/50000]\n",
      "loss:2.319661 [48000/50000]\n",
      "loss:2.321106 [48640/50000]\n",
      "loss:2.306165 [49280/50000]\n",
      "loss:2.309110 [49920/50000]\n",
      "test error:\n",
      " accuracy:10.0%,avg loss: 2.311048 \n",
      "\n",
      "epoch 2 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.300664 [    0/50000]\n",
      "loss:2.314545 [  640/50000]\n",
      "loss:2.302664 [ 1280/50000]\n",
      "loss:2.301002 [ 1920/50000]\n",
      "loss:2.301769 [ 2560/50000]\n",
      "loss:2.317119 [ 3200/50000]\n",
      "loss:2.295262 [ 3840/50000]\n",
      "loss:2.323356 [ 4480/50000]\n",
      "loss:2.310981 [ 5120/50000]\n",
      "loss:2.316601 [ 5760/50000]\n",
      "loss:2.302392 [ 6400/50000]\n",
      "loss:2.333488 [ 7040/50000]\n",
      "loss:2.305770 [ 7680/50000]\n",
      "loss:2.296818 [ 8320/50000]\n",
      "loss:2.304933 [ 8960/50000]\n",
      "loss:2.307060 [ 9600/50000]\n",
      "loss:2.283126 [10240/50000]\n",
      "loss:2.317814 [10880/50000]\n",
      "loss:2.300178 [11520/50000]\n",
      "loss:2.316799 [12160/50000]\n",
      "loss:2.303041 [12800/50000]\n",
      "loss:2.307287 [13440/50000]\n",
      "loss:2.288564 [14080/50000]\n",
      "loss:2.328963 [14720/50000]\n",
      "loss:2.328813 [15360/50000]\n",
      "loss:2.315938 [16000/50000]\n",
      "loss:2.313298 [16640/50000]\n",
      "loss:2.327547 [17280/50000]\n",
      "loss:2.299865 [17920/50000]\n",
      "loss:2.332420 [18560/50000]\n",
      "loss:2.299855 [19200/50000]\n",
      "loss:2.312116 [19840/50000]\n",
      "loss:2.300899 [20480/50000]\n",
      "loss:2.325742 [21120/50000]\n",
      "loss:2.295668 [21760/50000]\n",
      "loss:2.303699 [22400/50000]\n",
      "loss:2.334083 [23040/50000]\n",
      "loss:2.321451 [23680/50000]\n",
      "loss:2.316694 [24320/50000]\n",
      "loss:2.343040 [24960/50000]\n",
      "loss:2.308452 [25600/50000]\n",
      "loss:2.310906 [26240/50000]\n",
      "loss:2.344312 [26880/50000]\n",
      "loss:2.315243 [27520/50000]\n",
      "loss:2.302972 [28160/50000]\n",
      "loss:2.332479 [28800/50000]\n",
      "loss:2.293581 [29440/50000]\n",
      "loss:2.293143 [30080/50000]\n",
      "loss:2.300098 [30720/50000]\n",
      "loss:2.318254 [31360/50000]\n",
      "loss:2.319801 [32000/50000]\n",
      "loss:2.296447 [32640/50000]\n",
      "loss:2.314383 [33280/50000]\n",
      "loss:2.321578 [33920/50000]\n",
      "loss:2.303756 [34560/50000]\n",
      "loss:2.296796 [35200/50000]\n",
      "loss:2.306639 [35840/50000]\n",
      "loss:2.307699 [36480/50000]\n",
      "loss:2.297549 [37120/50000]\n",
      "loss:2.298483 [37760/50000]\n",
      "loss:2.329327 [38400/50000]\n",
      "loss:2.319343 [39040/50000]\n",
      "loss:2.325432 [39680/50000]\n",
      "loss:2.309486 [40320/50000]\n",
      "loss:2.281698 [40960/50000]\n",
      "loss:2.297707 [41600/50000]\n",
      "loss:2.302359 [42240/50000]\n",
      "loss:2.294897 [42880/50000]\n",
      "loss:2.343919 [43520/50000]\n",
      "loss:2.302776 [44160/50000]\n",
      "loss:2.311603 [44800/50000]\n",
      "loss:2.289241 [45440/50000]\n",
      "loss:2.299572 [46080/50000]\n",
      "loss:2.289397 [46720/50000]\n",
      "loss:2.299005 [47360/50000]\n",
      "loss:2.288797 [48000/50000]\n",
      "loss:2.346058 [48640/50000]\n",
      "loss:2.319704 [49280/50000]\n",
      "loss:2.328235 [49920/50000]\n",
      "test error:\n",
      " accuracy:10.0%,avg loss: 2.308191 \n",
      "\n",
      "epoch 3 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.289372 [    0/50000]\n",
      "loss:2.336305 [  640/50000]\n",
      "loss:2.316812 [ 1280/50000]\n",
      "loss:2.313157 [ 1920/50000]\n",
      "loss:2.301497 [ 2560/50000]\n",
      "loss:2.314988 [ 3200/50000]\n",
      "loss:2.298518 [ 3840/50000]\n",
      "loss:2.310969 [ 4480/50000]\n",
      "loss:2.289015 [ 5120/50000]\n",
      "loss:2.311166 [ 5760/50000]\n",
      "loss:2.322835 [ 6400/50000]\n",
      "loss:2.299688 [ 7040/50000]\n",
      "loss:2.304354 [ 7680/50000]\n",
      "loss:2.309251 [ 8320/50000]\n",
      "loss:2.319501 [ 8960/50000]\n",
      "loss:2.310031 [ 9600/50000]\n",
      "loss:2.298528 [10240/50000]\n",
      "loss:2.289974 [10880/50000]\n",
      "loss:2.301022 [11520/50000]\n",
      "loss:2.299282 [12160/50000]\n",
      "loss:2.326617 [12800/50000]\n",
      "loss:2.305605 [13440/50000]\n",
      "loss:2.290321 [14080/50000]\n",
      "loss:2.304663 [14720/50000]\n",
      "loss:2.324749 [15360/50000]\n",
      "loss:2.297039 [16000/50000]\n",
      "loss:2.316684 [16640/50000]\n",
      "loss:2.291321 [17280/50000]\n",
      "loss:2.269412 [17920/50000]\n",
      "loss:2.314352 [18560/50000]\n",
      "loss:2.312279 [19200/50000]\n",
      "loss:2.283004 [19840/50000]\n",
      "loss:2.299376 [20480/50000]\n",
      "loss:2.303802 [21120/50000]\n",
      "loss:2.320107 [21760/50000]\n",
      "loss:2.300252 [22400/50000]\n",
      "loss:2.316984 [23040/50000]\n",
      "loss:2.315966 [23680/50000]\n",
      "loss:2.294392 [24320/50000]\n",
      "loss:2.338961 [24960/50000]\n",
      "loss:2.291211 [25600/50000]\n",
      "loss:2.307238 [26240/50000]\n",
      "loss:2.301422 [26880/50000]\n",
      "loss:2.313248 [27520/50000]\n",
      "loss:2.314973 [28160/50000]\n",
      "loss:2.300385 [28800/50000]\n",
      "loss:2.295721 [29440/50000]\n",
      "loss:2.319762 [30080/50000]\n",
      "loss:2.349039 [30720/50000]\n",
      "loss:2.294690 [31360/50000]\n",
      "loss:2.307799 [32000/50000]\n",
      "loss:2.346313 [32640/50000]\n",
      "loss:2.300343 [33280/50000]\n",
      "loss:2.322969 [33920/50000]\n",
      "loss:2.302728 [34560/50000]\n",
      "loss:2.323427 [35200/50000]\n",
      "loss:2.314830 [35840/50000]\n",
      "loss:2.309750 [36480/50000]\n",
      "loss:2.291603 [37120/50000]\n",
      "loss:2.298203 [37760/50000]\n",
      "loss:2.299523 [38400/50000]\n",
      "loss:2.308126 [39040/50000]\n",
      "loss:2.301558 [39680/50000]\n",
      "loss:2.303615 [40320/50000]\n",
      "loss:2.287137 [40960/50000]\n",
      "loss:2.288822 [41600/50000]\n",
      "loss:2.307806 [42240/50000]\n",
      "loss:2.319924 [42880/50000]\n",
      "loss:2.323400 [43520/50000]\n",
      "loss:2.287868 [44160/50000]\n",
      "loss:2.296472 [44800/50000]\n",
      "loss:2.319564 [45440/50000]\n",
      "loss:2.307508 [46080/50000]\n",
      "loss:2.345422 [46720/50000]\n",
      "loss:2.313603 [47360/50000]\n",
      "loss:2.300221 [48000/50000]\n",
      "loss:2.323977 [48640/50000]\n",
      "loss:2.308163 [49280/50000]\n",
      "loss:2.312578 [49920/50000]\n",
      "test error:\n",
      " accuracy:10.0%,avg loss: 2.313724 \n",
      "\n",
      "epoch 4 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.336637 [    0/50000]\n",
      "loss:2.303296 [  640/50000]\n",
      "loss:2.295672 [ 1280/50000]\n",
      "loss:2.320524 [ 1920/50000]\n",
      "loss:2.307173 [ 2560/50000]\n",
      "loss:2.286189 [ 3200/50000]\n",
      "loss:2.295868 [ 3840/50000]\n",
      "loss:2.334042 [ 4480/50000]\n",
      "loss:2.300963 [ 5120/50000]\n",
      "loss:2.313654 [ 5760/50000]\n",
      "loss:2.292796 [ 6400/50000]\n",
      "loss:2.296141 [ 7040/50000]\n",
      "loss:2.330020 [ 7680/50000]\n",
      "loss:2.298249 [ 8320/50000]\n",
      "loss:2.305578 [ 8960/50000]\n",
      "loss:2.324800 [ 9600/50000]\n",
      "loss:2.290112 [10240/50000]\n",
      "loss:2.324826 [10880/50000]\n",
      "loss:2.302678 [11520/50000]\n",
      "loss:2.302625 [12160/50000]\n",
      "loss:2.296952 [12800/50000]\n",
      "loss:2.308947 [13440/50000]\n",
      "loss:2.331095 [14080/50000]\n",
      "loss:2.292563 [14720/50000]\n",
      "loss:2.286097 [15360/50000]\n",
      "loss:2.300678 [16000/50000]\n",
      "loss:2.297527 [16640/50000]\n",
      "loss:2.302662 [17280/50000]\n",
      "loss:2.300155 [17920/50000]\n",
      "loss:2.286992 [18560/50000]\n",
      "loss:2.303511 [19200/50000]\n",
      "loss:2.315695 [19840/50000]\n",
      "loss:2.286233 [20480/50000]\n",
      "loss:2.296421 [21120/50000]\n",
      "loss:2.313297 [21760/50000]\n",
      "loss:2.293609 [22400/50000]\n",
      "loss:2.295755 [23040/50000]\n",
      "loss:2.313139 [23680/50000]\n",
      "loss:2.292621 [24320/50000]\n",
      "loss:2.310844 [24960/50000]\n",
      "loss:2.316926 [25600/50000]\n",
      "loss:2.329822 [26240/50000]\n",
      "loss:2.304514 [26880/50000]\n",
      "loss:2.303238 [27520/50000]\n",
      "loss:2.293193 [28160/50000]\n",
      "loss:2.327773 [28800/50000]\n",
      "loss:2.294976 [29440/50000]\n",
      "loss:2.302887 [30080/50000]\n",
      "loss:2.293488 [30720/50000]\n",
      "loss:2.279004 [31360/50000]\n",
      "loss:2.294442 [32000/50000]\n",
      "loss:2.340892 [32640/50000]\n",
      "loss:2.286867 [33280/50000]\n",
      "loss:2.288804 [33920/50000]\n",
      "loss:2.295580 [34560/50000]\n",
      "loss:2.298127 [35200/50000]\n",
      "loss:2.287516 [35840/50000]\n",
      "loss:2.310479 [36480/50000]\n",
      "loss:2.285561 [37120/50000]\n",
      "loss:2.301550 [37760/50000]\n",
      "loss:2.312094 [38400/50000]\n",
      "loss:2.300559 [39040/50000]\n",
      "loss:2.333320 [39680/50000]\n",
      "loss:2.281129 [40320/50000]\n",
      "loss:2.292083 [40960/50000]\n",
      "loss:2.304636 [41600/50000]\n",
      "loss:2.311099 [42240/50000]\n",
      "loss:2.307970 [42880/50000]\n",
      "loss:2.306796 [43520/50000]\n",
      "loss:2.308815 [44160/50000]\n",
      "loss:2.299428 [44800/50000]\n",
      "loss:2.300012 [45440/50000]\n",
      "loss:2.287413 [46080/50000]\n",
      "loss:2.324893 [46720/50000]\n",
      "loss:2.308400 [47360/50000]\n",
      "loss:2.291358 [48000/50000]\n",
      "loss:2.313720 [48640/50000]\n",
      "loss:2.299783 [49280/50000]\n",
      "loss:2.315383 [49920/50000]\n",
      "test error:\n",
      " accuracy:11.1%,avg loss: 2.302936 \n",
      "\n",
      "epoch 5 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.295994 [    0/50000]\n",
      "loss:2.324480 [  640/50000]\n",
      "loss:2.295331 [ 1280/50000]\n",
      "loss:2.309970 [ 1920/50000]\n",
      "loss:2.297676 [ 2560/50000]\n",
      "loss:2.298211 [ 3200/50000]\n",
      "loss:2.307325 [ 3840/50000]\n",
      "loss:2.314958 [ 4480/50000]\n",
      "loss:2.309279 [ 5120/50000]\n",
      "loss:2.291766 [ 5760/50000]\n",
      "loss:2.300478 [ 6400/50000]\n",
      "loss:2.324503 [ 7040/50000]\n",
      "loss:2.295951 [ 7680/50000]\n",
      "loss:2.289379 [ 8320/50000]\n",
      "loss:2.286342 [ 8960/50000]\n",
      "loss:2.297851 [ 9600/50000]\n",
      "loss:2.293288 [10240/50000]\n",
      "loss:2.294165 [10880/50000]\n",
      "loss:2.310539 [11520/50000]\n",
      "loss:2.304126 [12160/50000]\n",
      "loss:2.289732 [12800/50000]\n",
      "loss:2.285467 [13440/50000]\n",
      "loss:2.299105 [14080/50000]\n",
      "loss:2.288346 [14720/50000]\n",
      "loss:2.270974 [15360/50000]\n",
      "loss:2.325328 [16000/50000]\n",
      "loss:2.290731 [16640/50000]\n",
      "loss:2.274270 [17280/50000]\n",
      "loss:2.285670 [17920/50000]\n",
      "loss:2.299678 [18560/50000]\n",
      "loss:2.276940 [19200/50000]\n",
      "loss:2.284338 [19840/50000]\n",
      "loss:2.297519 [20480/50000]\n",
      "loss:2.292490 [21120/50000]\n",
      "loss:2.282651 [21760/50000]\n",
      "loss:2.264689 [22400/50000]\n",
      "loss:2.279867 [23040/50000]\n",
      "loss:2.294874 [23680/50000]\n",
      "loss:2.291568 [24320/50000]\n",
      "loss:2.282080 [24960/50000]\n",
      "loss:2.275134 [25600/50000]\n",
      "loss:2.265552 [26240/50000]\n",
      "loss:2.283264 [26880/50000]\n",
      "loss:2.273887 [27520/50000]\n",
      "loss:2.296146 [28160/50000]\n",
      "loss:2.261098 [28800/50000]\n",
      "loss:2.266640 [29440/50000]\n",
      "loss:2.281139 [30080/50000]\n",
      "loss:2.264192 [30720/50000]\n",
      "loss:2.263695 [31360/50000]\n",
      "loss:2.227801 [32000/50000]\n",
      "loss:2.260935 [32640/50000]\n",
      "loss:2.215148 [33280/50000]\n",
      "loss:2.256674 [33920/50000]\n",
      "loss:2.227610 [34560/50000]\n",
      "loss:2.222412 [35200/50000]\n",
      "loss:2.274381 [35840/50000]\n",
      "loss:2.208724 [36480/50000]\n",
      "loss:2.246760 [37120/50000]\n",
      "loss:2.196308 [37760/50000]\n",
      "loss:2.214472 [38400/50000]\n",
      "loss:2.269612 [39040/50000]\n",
      "loss:2.187903 [39680/50000]\n",
      "loss:2.235975 [40320/50000]\n",
      "loss:2.181363 [40960/50000]\n",
      "loss:2.214913 [41600/50000]\n",
      "loss:2.238611 [42240/50000]\n",
      "loss:2.216274 [42880/50000]\n",
      "loss:2.158963 [43520/50000]\n",
      "loss:2.210939 [44160/50000]\n",
      "loss:2.110624 [44800/50000]\n",
      "loss:2.130083 [45440/50000]\n",
      "loss:2.150545 [46080/50000]\n",
      "loss:2.159805 [46720/50000]\n",
      "loss:2.290225 [47360/50000]\n",
      "loss:2.081033 [48000/50000]\n",
      "loss:2.239450 [48640/50000]\n",
      "loss:2.116989 [49280/50000]\n",
      "loss:2.215173 [49920/50000]\n",
      "test error:\n",
      " accuracy:13.0%,avg loss: 2.357171 \n",
      "\n",
      "epoch 6 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.423168 [    0/50000]\n",
      "loss:2.111254 [  640/50000]\n",
      "loss:2.105636 [ 1280/50000]\n",
      "loss:2.035378 [ 1920/50000]\n",
      "loss:2.061563 [ 2560/50000]\n",
      "loss:2.223433 [ 3200/50000]\n",
      "loss:2.095504 [ 3840/50000]\n",
      "loss:2.038181 [ 4480/50000]\n",
      "loss:2.067997 [ 5120/50000]\n",
      "loss:2.236157 [ 5760/50000]\n",
      "loss:2.045677 [ 6400/50000]\n",
      "loss:2.049912 [ 7040/50000]\n",
      "loss:2.042103 [ 7680/50000]\n",
      "loss:2.097729 [ 8320/50000]\n",
      "loss:2.059892 [ 8960/50000]\n",
      "loss:1.968489 [ 9600/50000]\n",
      "loss:2.117961 [10240/50000]\n",
      "loss:2.056442 [10880/50000]\n",
      "loss:2.018971 [11520/50000]\n",
      "loss:1.996682 [12160/50000]\n",
      "loss:2.172174 [12800/50000]\n",
      "loss:2.175821 [13440/50000]\n",
      "loss:2.201419 [14080/50000]\n",
      "loss:2.059504 [14720/50000]\n",
      "loss:2.134009 [15360/50000]\n",
      "loss:2.099293 [16000/50000]\n",
      "loss:2.072538 [16640/50000]\n",
      "loss:2.077919 [17280/50000]\n",
      "loss:2.089740 [17920/50000]\n",
      "loss:2.036705 [18560/50000]\n",
      "loss:2.168776 [19200/50000]\n",
      "loss:2.158464 [19840/50000]\n",
      "loss:2.094300 [20480/50000]\n",
      "loss:2.104140 [21120/50000]\n",
      "loss:2.106270 [21760/50000]\n",
      "loss:2.063648 [22400/50000]\n",
      "loss:2.167919 [23040/50000]\n",
      "loss:2.007617 [23680/50000]\n",
      "loss:2.058273 [24320/50000]\n",
      "loss:2.128674 [24960/50000]\n",
      "loss:2.038256 [25600/50000]\n",
      "loss:2.055499 [26240/50000]\n",
      "loss:2.071708 [26880/50000]\n",
      "loss:2.097473 [27520/50000]\n",
      "loss:2.062265 [28160/50000]\n",
      "loss:2.037192 [28800/50000]\n",
      "loss:2.085409 [29440/50000]\n",
      "loss:2.011938 [30080/50000]\n",
      "loss:2.217662 [30720/50000]\n",
      "loss:1.988605 [31360/50000]\n",
      "loss:1.938632 [32000/50000]\n",
      "loss:1.976193 [32640/50000]\n",
      "loss:2.083410 [33280/50000]\n",
      "loss:1.939476 [33920/50000]\n",
      "loss:2.085046 [34560/50000]\n",
      "loss:2.063265 [35200/50000]\n",
      "loss:2.204410 [35840/50000]\n",
      "loss:2.085499 [36480/50000]\n",
      "loss:2.127533 [37120/50000]\n",
      "loss:2.012964 [37760/50000]\n",
      "loss:2.090620 [38400/50000]\n",
      "loss:2.162423 [39040/50000]\n",
      "loss:2.007418 [39680/50000]\n",
      "loss:2.118328 [40320/50000]\n",
      "loss:2.115566 [40960/50000]\n",
      "loss:2.072465 [41600/50000]\n",
      "loss:2.089908 [42240/50000]\n",
      "loss:2.020634 [42880/50000]\n",
      "loss:1.984397 [43520/50000]\n",
      "loss:1.930682 [44160/50000]\n",
      "loss:2.145360 [44800/50000]\n",
      "loss:1.996622 [45440/50000]\n",
      "loss:1.981509 [46080/50000]\n",
      "loss:2.024519 [46720/50000]\n",
      "loss:2.129475 [47360/50000]\n",
      "loss:2.107388 [48000/50000]\n",
      "loss:1.965257 [48640/50000]\n",
      "loss:2.056762 [49280/50000]\n",
      "loss:2.036361 [49920/50000]\n",
      "test error:\n",
      " accuracy:22.7%,avg loss: 2.046830 \n",
      "\n",
      "epoch 7 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.074986 [    0/50000]\n",
      "loss:1.923631 [  640/50000]\n",
      "loss:2.091159 [ 1280/50000]\n",
      "loss:1.953668 [ 1920/50000]\n",
      "loss:1.970555 [ 2560/50000]\n",
      "loss:2.117952 [ 3200/50000]\n",
      "loss:1.991459 [ 3840/50000]\n",
      "loss:2.010246 [ 4480/50000]\n",
      "loss:1.985839 [ 5120/50000]\n",
      "loss:2.035115 [ 5760/50000]\n",
      "loss:1.962869 [ 6400/50000]\n",
      "loss:2.105664 [ 7040/50000]\n",
      "loss:2.037663 [ 7680/50000]\n",
      "loss:2.143440 [ 8320/50000]\n",
      "loss:2.092505 [ 8960/50000]\n",
      "loss:1.957326 [ 9600/50000]\n",
      "loss:2.013082 [10240/50000]\n",
      "loss:2.093812 [10880/50000]\n",
      "loss:2.119968 [11520/50000]\n",
      "loss:2.055893 [12160/50000]\n",
      "loss:2.213552 [12800/50000]\n",
      "loss:2.067643 [13440/50000]\n",
      "loss:2.014232 [14080/50000]\n",
      "loss:2.132733 [14720/50000]\n",
      "loss:1.982734 [15360/50000]\n",
      "loss:2.087097 [16000/50000]\n",
      "loss:2.004762 [16640/50000]\n",
      "loss:1.924274 [17280/50000]\n",
      "loss:2.044685 [17920/50000]\n",
      "loss:2.061311 [18560/50000]\n",
      "loss:2.117390 [19200/50000]\n",
      "loss:2.135067 [19840/50000]\n",
      "loss:2.052266 [20480/50000]\n",
      "loss:2.151043 [21120/50000]\n",
      "loss:1.981797 [21760/50000]\n",
      "loss:2.017329 [22400/50000]\n",
      "loss:1.945799 [23040/50000]\n",
      "loss:1.977625 [23680/50000]\n",
      "loss:1.953479 [24320/50000]\n",
      "loss:2.039704 [24960/50000]\n",
      "loss:1.970973 [25600/50000]\n",
      "loss:2.069012 [26240/50000]\n",
      "loss:1.999382 [26880/50000]\n",
      "loss:2.022941 [27520/50000]\n",
      "loss:1.904750 [28160/50000]\n",
      "loss:2.019795 [28800/50000]\n",
      "loss:2.023051 [29440/50000]\n",
      "loss:2.081021 [30080/50000]\n",
      "loss:1.877315 [30720/50000]\n",
      "loss:1.997736 [31360/50000]\n",
      "loss:2.016543 [32000/50000]\n",
      "loss:2.168851 [32640/50000]\n",
      "loss:1.981160 [33280/50000]\n",
      "loss:2.117662 [33920/50000]\n",
      "loss:2.083405 [34560/50000]\n",
      "loss:1.959689 [35200/50000]\n",
      "loss:2.045945 [35840/50000]\n",
      "loss:2.084158 [36480/50000]\n",
      "loss:2.054760 [37120/50000]\n",
      "loss:2.056216 [37760/50000]\n",
      "loss:1.989757 [38400/50000]\n",
      "loss:2.033667 [39040/50000]\n",
      "loss:2.112373 [39680/50000]\n",
      "loss:2.079642 [40320/50000]\n",
      "loss:1.947566 [40960/50000]\n",
      "loss:1.955891 [41600/50000]\n",
      "loss:2.044343 [42240/50000]\n",
      "loss:2.005661 [42880/50000]\n",
      "loss:2.003647 [43520/50000]\n",
      "loss:2.152667 [44160/50000]\n",
      "loss:1.961563 [44800/50000]\n",
      "loss:1.975007 [45440/50000]\n",
      "loss:2.048156 [46080/50000]\n",
      "loss:1.976108 [46720/50000]\n",
      "loss:2.072262 [47360/50000]\n",
      "loss:2.093116 [48000/50000]\n",
      "loss:1.943167 [48640/50000]\n",
      "loss:2.143622 [49280/50000]\n",
      "loss:2.079785 [49920/50000]\n",
      "test error:\n",
      " accuracy:24.1%,avg loss: 2.069745 \n",
      "\n",
      "epoch 8 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.128742 [    0/50000]\n",
      "loss:2.060163 [  640/50000]\n",
      "loss:2.142313 [ 1280/50000]\n",
      "loss:2.011366 [ 1920/50000]\n",
      "loss:1.827426 [ 2560/50000]\n",
      "loss:2.056047 [ 3200/50000]\n",
      "loss:1.938736 [ 3840/50000]\n",
      "loss:1.996233 [ 4480/50000]\n",
      "loss:1.973259 [ 5120/50000]\n",
      "loss:2.091208 [ 5760/50000]\n",
      "loss:2.017050 [ 6400/50000]\n",
      "loss:2.124194 [ 7040/50000]\n",
      "loss:2.013343 [ 7680/50000]\n",
      "loss:2.059035 [ 8320/50000]\n",
      "loss:1.957593 [ 8960/50000]\n",
      "loss:1.959980 [ 9600/50000]\n",
      "loss:1.866568 [10240/50000]\n",
      "loss:2.081369 [10880/50000]\n",
      "loss:2.002458 [11520/50000]\n",
      "loss:2.183537 [12160/50000]\n",
      "loss:2.015282 [12800/50000]\n",
      "loss:1.977562 [13440/50000]\n",
      "loss:2.070004 [14080/50000]\n",
      "loss:2.066395 [14720/50000]\n",
      "loss:1.944305 [15360/50000]\n",
      "loss:2.100159 [16000/50000]\n",
      "loss:2.100366 [16640/50000]\n",
      "loss:1.875203 [17280/50000]\n",
      "loss:1.972224 [17920/50000]\n",
      "loss:2.022264 [18560/50000]\n",
      "loss:2.116765 [19200/50000]\n",
      "loss:1.949748 [19840/50000]\n",
      "loss:2.088958 [20480/50000]\n",
      "loss:2.028906 [21120/50000]\n",
      "loss:1.947842 [21760/50000]\n",
      "loss:1.955874 [22400/50000]\n",
      "loss:2.055736 [23040/50000]\n",
      "loss:1.981264 [23680/50000]\n",
      "loss:1.922259 [24320/50000]\n",
      "loss:1.996393 [24960/50000]\n",
      "loss:2.104735 [25600/50000]\n",
      "loss:1.982005 [26240/50000]\n",
      "loss:2.095835 [26880/50000]\n",
      "loss:2.091396 [27520/50000]\n",
      "loss:2.210381 [28160/50000]\n",
      "loss:2.154696 [28800/50000]\n",
      "loss:2.141152 [29440/50000]\n",
      "loss:1.897900 [30080/50000]\n",
      "loss:1.926832 [30720/50000]\n",
      "loss:2.052600 [31360/50000]\n",
      "loss:1.895730 [32000/50000]\n",
      "loss:1.959652 [32640/50000]\n",
      "loss:1.872807 [33280/50000]\n",
      "loss:2.029766 [33920/50000]\n",
      "loss:1.992465 [34560/50000]\n",
      "loss:1.862515 [35200/50000]\n",
      "loss:1.948346 [35840/50000]\n",
      "loss:2.166871 [36480/50000]\n",
      "loss:2.154843 [37120/50000]\n",
      "loss:2.094136 [37760/50000]\n",
      "loss:1.769164 [38400/50000]\n",
      "loss:2.131179 [39040/50000]\n",
      "loss:1.947673 [39680/50000]\n",
      "loss:2.029824 [40320/50000]\n",
      "loss:1.986112 [40960/50000]\n",
      "loss:1.887757 [41600/50000]\n",
      "loss:2.017297 [42240/50000]\n",
      "loss:1.984232 [42880/50000]\n",
      "loss:2.069156 [43520/50000]\n",
      "loss:2.057234 [44160/50000]\n",
      "loss:1.947808 [44800/50000]\n",
      "loss:2.061494 [45440/50000]\n",
      "loss:1.891744 [46080/50000]\n",
      "loss:1.959491 [46720/50000]\n",
      "loss:1.963345 [47360/50000]\n",
      "loss:1.925810 [48000/50000]\n",
      "loss:1.949524 [48640/50000]\n",
      "loss:2.067148 [49280/50000]\n",
      "loss:1.934224 [49920/50000]\n",
      "test error:\n",
      " accuracy:25.3%,avg loss: 2.016821 \n",
      "\n",
      "epoch 9 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:2.080207 [    0/50000]\n",
      "loss:1.895434 [  640/50000]\n",
      "loss:2.023404 [ 1280/50000]\n",
      "loss:2.104984 [ 1920/50000]\n",
      "loss:1.938793 [ 2560/50000]\n",
      "loss:1.828998 [ 3200/50000]\n",
      "loss:1.973336 [ 3840/50000]\n",
      "loss:1.896142 [ 4480/50000]\n",
      "loss:1.906227 [ 5120/50000]\n",
      "loss:1.996380 [ 5760/50000]\n",
      "loss:2.078160 [ 6400/50000]\n",
      "loss:2.026613 [ 7040/50000]\n",
      "loss:2.064592 [ 7680/50000]\n",
      "loss:1.838945 [ 8320/50000]\n",
      "loss:1.991373 [ 8960/50000]\n",
      "loss:2.048219 [ 9600/50000]\n",
      "loss:1.982497 [10240/50000]\n",
      "loss:2.020428 [10880/50000]\n",
      "loss:1.988059 [11520/50000]\n",
      "loss:1.966538 [12160/50000]\n",
      "loss:2.019262 [12800/50000]\n",
      "loss:1.914822 [13440/50000]\n",
      "loss:1.877196 [14080/50000]\n",
      "loss:2.016967 [14720/50000]\n",
      "loss:1.867639 [15360/50000]\n",
      "loss:1.863068 [16000/50000]\n",
      "loss:1.859091 [16640/50000]\n",
      "loss:2.026295 [17280/50000]\n",
      "loss:1.983984 [17920/50000]\n",
      "loss:1.939926 [18560/50000]\n",
      "loss:2.024470 [19200/50000]\n",
      "loss:1.937969 [19840/50000]\n",
      "loss:1.978911 [20480/50000]\n",
      "loss:1.995328 [21120/50000]\n",
      "loss:1.932936 [21760/50000]\n",
      "loss:1.961627 [22400/50000]\n",
      "loss:2.078758 [23040/50000]\n",
      "loss:1.896040 [23680/50000]\n",
      "loss:1.852470 [24320/50000]\n",
      "loss:1.994587 [24960/50000]\n",
      "loss:2.026513 [25600/50000]\n",
      "loss:1.862527 [26240/50000]\n",
      "loss:1.898404 [26880/50000]\n",
      "loss:1.955938 [27520/50000]\n",
      "loss:2.054909 [28160/50000]\n",
      "loss:1.873220 [28800/50000]\n",
      "loss:1.857473 [29440/50000]\n",
      "loss:2.030563 [30080/50000]\n",
      "loss:2.003342 [30720/50000]\n",
      "loss:1.853165 [31360/50000]\n",
      "loss:2.061484 [32000/50000]\n",
      "loss:1.963381 [32640/50000]\n",
      "loss:1.879650 [33280/50000]\n",
      "loss:1.989298 [33920/50000]\n",
      "loss:2.084997 [34560/50000]\n",
      "loss:1.852115 [35200/50000]\n",
      "loss:1.950508 [35840/50000]\n",
      "loss:2.124746 [36480/50000]\n",
      "loss:1.934965 [37120/50000]\n",
      "loss:1.963799 [37760/50000]\n",
      "loss:1.933743 [38400/50000]\n",
      "loss:1.934235 [39040/50000]\n",
      "loss:1.994225 [39680/50000]\n",
      "loss:1.998531 [40320/50000]\n",
      "loss:1.959461 [40960/50000]\n",
      "loss:2.049747 [41600/50000]\n",
      "loss:1.849717 [42240/50000]\n",
      "loss:1.970719 [42880/50000]\n",
      "loss:2.023580 [43520/50000]\n",
      "loss:2.031249 [44160/50000]\n",
      "loss:2.042452 [44800/50000]\n",
      "loss:1.983758 [45440/50000]\n",
      "loss:1.989169 [46080/50000]\n",
      "loss:1.891545 [46720/50000]\n",
      "loss:1.966013 [47360/50000]\n",
      "loss:1.936801 [48000/50000]\n",
      "loss:1.980172 [48640/50000]\n",
      "loss:1.811807 [49280/50000]\n",
      "loss:2.111814 [49920/50000]\n",
      "test error:\n",
      " accuracy:26.0%,avg loss: 1.980274 \n",
      "\n",
      "epoch 10 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.895540 [    0/50000]\n",
      "loss:1.879374 [  640/50000]\n",
      "loss:2.092981 [ 1280/50000]\n",
      "loss:1.938980 [ 1920/50000]\n",
      "loss:1.954616 [ 2560/50000]\n",
      "loss:2.114228 [ 3200/50000]\n",
      "loss:1.845685 [ 3840/50000]\n",
      "loss:1.856801 [ 4480/50000]\n",
      "loss:2.060495 [ 5120/50000]\n",
      "loss:1.993286 [ 5760/50000]\n",
      "loss:1.899745 [ 6400/50000]\n",
      "loss:1.907701 [ 7040/50000]\n",
      "loss:1.960093 [ 7680/50000]\n",
      "loss:2.139092 [ 8320/50000]\n",
      "loss:1.899080 [ 8960/50000]\n",
      "loss:1.808708 [ 9600/50000]\n",
      "loss:1.945888 [10240/50000]\n",
      "loss:1.979757 [10880/50000]\n",
      "loss:1.961669 [11520/50000]\n",
      "loss:1.979236 [12160/50000]\n",
      "loss:1.853390 [12800/50000]\n",
      "loss:1.974239 [13440/50000]\n",
      "loss:2.095164 [14080/50000]\n",
      "loss:1.899338 [14720/50000]\n",
      "loss:2.036541 [15360/50000]\n",
      "loss:2.008595 [16000/50000]\n",
      "loss:2.045726 [16640/50000]\n",
      "loss:1.884889 [17280/50000]\n",
      "loss:1.899811 [17920/50000]\n",
      "loss:1.885613 [18560/50000]\n",
      "loss:1.909544 [19200/50000]\n",
      "loss:1.915852 [19840/50000]\n",
      "loss:1.832344 [20480/50000]\n",
      "loss:2.057068 [21120/50000]\n",
      "loss:1.861135 [21760/50000]\n",
      "loss:1.966175 [22400/50000]\n",
      "loss:2.026184 [23040/50000]\n",
      "loss:2.124967 [23680/50000]\n",
      "loss:1.909217 [24320/50000]\n",
      "loss:1.820323 [24960/50000]\n",
      "loss:1.851149 [25600/50000]\n",
      "loss:1.942994 [26240/50000]\n",
      "loss:2.024543 [26880/50000]\n",
      "loss:2.029274 [27520/50000]\n",
      "loss:1.977889 [28160/50000]\n",
      "loss:1.991549 [28800/50000]\n",
      "loss:1.868785 [29440/50000]\n",
      "loss:2.112927 [30080/50000]\n",
      "loss:1.970415 [30720/50000]\n",
      "loss:1.924464 [31360/50000]\n",
      "loss:2.008461 [32000/50000]\n",
      "loss:2.056886 [32640/50000]\n",
      "loss:1.894190 [33280/50000]\n",
      "loss:1.887239 [33920/50000]\n",
      "loss:2.036224 [34560/50000]\n",
      "loss:2.054853 [35200/50000]\n",
      "loss:2.011293 [35840/50000]\n",
      "loss:2.020228 [36480/50000]\n",
      "loss:2.031182 [37120/50000]\n",
      "loss:2.062759 [37760/50000]\n",
      "loss:2.029912 [38400/50000]\n",
      "loss:2.045166 [39040/50000]\n",
      "loss:1.973414 [39680/50000]\n",
      "loss:1.866181 [40320/50000]\n",
      "loss:1.997270 [40960/50000]\n",
      "loss:2.027676 [41600/50000]\n",
      "loss:1.890292 [42240/50000]\n",
      "loss:2.057060 [42880/50000]\n",
      "loss:1.923672 [43520/50000]\n",
      "loss:1.898222 [44160/50000]\n",
      "loss:1.964965 [44800/50000]\n",
      "loss:1.937546 [45440/50000]\n",
      "loss:1.928288 [46080/50000]\n",
      "loss:1.840007 [46720/50000]\n",
      "loss:1.891073 [47360/50000]\n",
      "loss:1.953094 [48000/50000]\n",
      "loss:2.069025 [48640/50000]\n",
      "loss:1.972660 [49280/50000]\n",
      "loss:1.948534 [49920/50000]\n",
      "test error:\n",
      " accuracy:26.9%,avg loss: 1.951111 \n",
      "\n",
      "epoch 11 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.827369 [    0/50000]\n",
      "loss:1.834502 [  640/50000]\n",
      "loss:1.752636 [ 1280/50000]\n",
      "loss:1.773909 [ 1920/50000]\n",
      "loss:1.936838 [ 2560/50000]\n",
      "loss:1.848780 [ 3200/50000]\n",
      "loss:1.928793 [ 3840/50000]\n",
      "loss:1.998460 [ 4480/50000]\n",
      "loss:1.961170 [ 5120/50000]\n",
      "loss:1.839673 [ 5760/50000]\n",
      "loss:1.982956 [ 6400/50000]\n",
      "loss:1.925508 [ 7040/50000]\n",
      "loss:1.861251 [ 7680/50000]\n",
      "loss:1.868195 [ 8320/50000]\n",
      "loss:2.018268 [ 8960/50000]\n",
      "loss:1.920116 [ 9600/50000]\n",
      "loss:1.954886 [10240/50000]\n",
      "loss:1.951794 [10880/50000]\n",
      "loss:1.997905 [11520/50000]\n",
      "loss:2.036246 [12160/50000]\n",
      "loss:2.002210 [12800/50000]\n",
      "loss:2.032456 [13440/50000]\n",
      "loss:2.058949 [14080/50000]\n",
      "loss:1.909656 [14720/50000]\n",
      "loss:1.856700 [15360/50000]\n",
      "loss:1.905160 [16000/50000]\n",
      "loss:2.007359 [16640/50000]\n",
      "loss:1.932343 [17280/50000]\n",
      "loss:1.834912 [17920/50000]\n",
      "loss:1.881092 [18560/50000]\n",
      "loss:1.834072 [19200/50000]\n",
      "loss:1.758329 [19840/50000]\n",
      "loss:1.714104 [20480/50000]\n",
      "loss:1.995584 [21120/50000]\n",
      "loss:1.994692 [21760/50000]\n",
      "loss:1.951812 [22400/50000]\n",
      "loss:2.025789 [23040/50000]\n",
      "loss:1.917858 [23680/50000]\n",
      "loss:2.011669 [24320/50000]\n",
      "loss:1.919878 [24960/50000]\n",
      "loss:1.953470 [25600/50000]\n",
      "loss:2.021115 [26240/50000]\n",
      "loss:1.845447 [26880/50000]\n",
      "loss:1.835220 [27520/50000]\n",
      "loss:1.853555 [28160/50000]\n",
      "loss:2.088751 [28800/50000]\n",
      "loss:1.962537 [29440/50000]\n",
      "loss:1.886822 [30080/50000]\n",
      "loss:1.982726 [30720/50000]\n",
      "loss:2.082163 [31360/50000]\n",
      "loss:2.093133 [32000/50000]\n",
      "loss:1.974285 [32640/50000]\n",
      "loss:1.758942 [33280/50000]\n",
      "loss:1.957268 [33920/50000]\n",
      "loss:1.835886 [34560/50000]\n",
      "loss:1.915682 [35200/50000]\n",
      "loss:1.882626 [35840/50000]\n",
      "loss:1.986320 [36480/50000]\n",
      "loss:2.064066 [37120/50000]\n",
      "loss:1.897085 [37760/50000]\n",
      "loss:1.977184 [38400/50000]\n",
      "loss:2.029270 [39040/50000]\n",
      "loss:1.970005 [39680/50000]\n",
      "loss:1.933334 [40320/50000]\n",
      "loss:2.037601 [40960/50000]\n",
      "loss:2.001934 [41600/50000]\n",
      "loss:1.820219 [42240/50000]\n",
      "loss:1.862672 [42880/50000]\n",
      "loss:1.827332 [43520/50000]\n",
      "loss:1.936087 [44160/50000]\n",
      "loss:2.074222 [44800/50000]\n",
      "loss:1.909476 [45440/50000]\n",
      "loss:2.011139 [46080/50000]\n",
      "loss:1.785190 [46720/50000]\n",
      "loss:1.859336 [47360/50000]\n",
      "loss:1.897719 [48000/50000]\n",
      "loss:1.910741 [48640/50000]\n",
      "loss:1.905910 [49280/50000]\n",
      "loss:1.872793 [49920/50000]\n",
      "test error:\n",
      " accuracy:23.8%,avg loss: 2.058206 \n",
      "\n",
      "epoch 12 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.981347 [    0/50000]\n",
      "loss:1.784165 [  640/50000]\n",
      "loss:1.744514 [ 1280/50000]\n",
      "loss:2.028126 [ 1920/50000]\n",
      "loss:1.851142 [ 2560/50000]\n",
      "loss:1.875376 [ 3200/50000]\n",
      "loss:1.941211 [ 3840/50000]\n",
      "loss:1.862638 [ 4480/50000]\n",
      "loss:1.786191 [ 5120/50000]\n",
      "loss:1.880633 [ 5760/50000]\n",
      "loss:1.865562 [ 6400/50000]\n",
      "loss:1.977470 [ 7040/50000]\n",
      "loss:1.831128 [ 7680/50000]\n",
      "loss:2.008519 [ 8320/50000]\n",
      "loss:1.804145 [ 8960/50000]\n",
      "loss:1.846960 [ 9600/50000]\n",
      "loss:1.919017 [10240/50000]\n",
      "loss:1.919197 [10880/50000]\n",
      "loss:2.057830 [11520/50000]\n",
      "loss:2.033428 [12160/50000]\n",
      "loss:1.840562 [12800/50000]\n",
      "loss:2.000095 [13440/50000]\n",
      "loss:1.897059 [14080/50000]\n",
      "loss:1.871731 [14720/50000]\n",
      "loss:1.750624 [15360/50000]\n",
      "loss:1.942846 [16000/50000]\n",
      "loss:1.729853 [16640/50000]\n",
      "loss:1.828920 [17280/50000]\n",
      "loss:2.015964 [17920/50000]\n",
      "loss:1.945191 [18560/50000]\n",
      "loss:2.032903 [19200/50000]\n",
      "loss:2.010179 [19840/50000]\n",
      "loss:1.739968 [20480/50000]\n",
      "loss:2.031198 [21120/50000]\n",
      "loss:1.841081 [21760/50000]\n",
      "loss:1.855228 [22400/50000]\n",
      "loss:2.078029 [23040/50000]\n",
      "loss:1.829444 [23680/50000]\n",
      "loss:1.911445 [24320/50000]\n",
      "loss:1.925621 [24960/50000]\n",
      "loss:1.993013 [25600/50000]\n",
      "loss:1.891204 [26240/50000]\n",
      "loss:1.878445 [26880/50000]\n",
      "loss:1.861932 [27520/50000]\n",
      "loss:1.937114 [28160/50000]\n",
      "loss:1.984739 [28800/50000]\n",
      "loss:2.115778 [29440/50000]\n",
      "loss:2.184775 [30080/50000]\n",
      "loss:1.894705 [30720/50000]\n",
      "loss:1.697655 [31360/50000]\n",
      "loss:1.917685 [32000/50000]\n",
      "loss:1.938092 [32640/50000]\n",
      "loss:1.881156 [33280/50000]\n",
      "loss:1.917055 [33920/50000]\n",
      "loss:1.674867 [34560/50000]\n",
      "loss:2.053296 [35200/50000]\n",
      "loss:1.912030 [35840/50000]\n",
      "loss:1.910666 [36480/50000]\n",
      "loss:1.946462 [37120/50000]\n",
      "loss:1.799762 [37760/50000]\n",
      "loss:1.877736 [38400/50000]\n",
      "loss:1.771671 [39040/50000]\n",
      "loss:1.847507 [39680/50000]\n",
      "loss:1.949798 [40320/50000]\n",
      "loss:1.895969 [40960/50000]\n",
      "loss:1.896834 [41600/50000]\n",
      "loss:1.955453 [42240/50000]\n",
      "loss:1.775705 [42880/50000]\n",
      "loss:1.931401 [43520/50000]\n",
      "loss:1.910460 [44160/50000]\n",
      "loss:1.800218 [44800/50000]\n",
      "loss:1.861673 [45440/50000]\n",
      "loss:1.912591 [46080/50000]\n",
      "loss:1.771068 [46720/50000]\n",
      "loss:1.793266 [47360/50000]\n",
      "loss:2.007824 [48000/50000]\n",
      "loss:1.858519 [48640/50000]\n",
      "loss:1.866284 [49280/50000]\n",
      "loss:1.920228 [49920/50000]\n",
      "test error:\n",
      " accuracy:25.9%,avg loss: 1.966131 \n",
      "\n",
      "epoch 13 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.751220 [    0/50000]\n",
      "loss:1.922130 [  640/50000]\n",
      "loss:1.886348 [ 1280/50000]\n",
      "loss:1.819046 [ 1920/50000]\n",
      "loss:1.831790 [ 2560/50000]\n",
      "loss:1.846477 [ 3200/50000]\n",
      "loss:1.879816 [ 3840/50000]\n",
      "loss:1.725092 [ 4480/50000]\n",
      "loss:1.955600 [ 5120/50000]\n",
      "loss:1.727807 [ 5760/50000]\n",
      "loss:1.771538 [ 6400/50000]\n",
      "loss:1.974852 [ 7040/50000]\n",
      "loss:1.858093 [ 7680/50000]\n",
      "loss:1.772856 [ 8320/50000]\n",
      "loss:1.760418 [ 8960/50000]\n",
      "loss:1.893759 [ 9600/50000]\n",
      "loss:1.868100 [10240/50000]\n",
      "loss:1.937053 [10880/50000]\n",
      "loss:1.940608 [11520/50000]\n",
      "loss:1.849834 [12160/50000]\n",
      "loss:1.938844 [12800/50000]\n",
      "loss:2.098686 [13440/50000]\n",
      "loss:1.997148 [14080/50000]\n",
      "loss:1.839918 [14720/50000]\n",
      "loss:1.873114 [15360/50000]\n",
      "loss:1.960890 [16000/50000]\n",
      "loss:1.910611 [16640/50000]\n",
      "loss:1.825997 [17280/50000]\n",
      "loss:1.864558 [17920/50000]\n",
      "loss:1.876938 [18560/50000]\n",
      "loss:1.952217 [19200/50000]\n",
      "loss:1.820939 [19840/50000]\n",
      "loss:1.760694 [20480/50000]\n",
      "loss:1.719544 [21120/50000]\n",
      "loss:1.980444 [21760/50000]\n",
      "loss:1.739019 [22400/50000]\n",
      "loss:1.926903 [23040/50000]\n",
      "loss:1.944881 [23680/50000]\n",
      "loss:1.899956 [24320/50000]\n",
      "loss:1.954424 [24960/50000]\n",
      "loss:2.012754 [25600/50000]\n",
      "loss:1.948045 [26240/50000]\n",
      "loss:1.960972 [26880/50000]\n",
      "loss:1.851339 [27520/50000]\n",
      "loss:1.856577 [28160/50000]\n",
      "loss:1.930794 [28800/50000]\n",
      "loss:1.712688 [29440/50000]\n",
      "loss:1.744264 [30080/50000]\n",
      "loss:1.745598 [30720/50000]\n",
      "loss:1.894827 [31360/50000]\n",
      "loss:1.730661 [32000/50000]\n",
      "loss:2.019598 [32640/50000]\n",
      "loss:2.063359 [33280/50000]\n",
      "loss:1.951903 [33920/50000]\n",
      "loss:1.938702 [34560/50000]\n",
      "loss:1.898404 [35200/50000]\n",
      "loss:1.721137 [35840/50000]\n",
      "loss:1.856875 [36480/50000]\n",
      "loss:1.720557 [37120/50000]\n",
      "loss:1.768522 [37760/50000]\n",
      "loss:1.662897 [38400/50000]\n",
      "loss:1.815562 [39040/50000]\n",
      "loss:1.888047 [39680/50000]\n",
      "loss:1.783269 [40320/50000]\n",
      "loss:1.944822 [40960/50000]\n",
      "loss:1.803508 [41600/50000]\n",
      "loss:1.758092 [42240/50000]\n",
      "loss:1.837934 [42880/50000]\n",
      "loss:1.707006 [43520/50000]\n",
      "loss:1.995514 [44160/50000]\n",
      "loss:1.746024 [44800/50000]\n",
      "loss:1.780600 [45440/50000]\n",
      "loss:1.709082 [46080/50000]\n",
      "loss:1.694015 [46720/50000]\n",
      "loss:1.864028 [47360/50000]\n",
      "loss:1.843621 [48000/50000]\n",
      "loss:1.894215 [48640/50000]\n",
      "loss:1.929783 [49280/50000]\n",
      "loss:1.960797 [49920/50000]\n",
      "test error:\n",
      " accuracy:31.6%,avg loss: 1.844947 \n",
      "\n",
      "epoch 14 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.763173 [    0/50000]\n",
      "loss:1.818520 [  640/50000]\n",
      "loss:1.903051 [ 1280/50000]\n",
      "loss:1.654893 [ 1920/50000]\n",
      "loss:1.751967 [ 2560/50000]\n",
      "loss:1.948442 [ 3200/50000]\n",
      "loss:1.901862 [ 3840/50000]\n",
      "loss:1.924344 [ 4480/50000]\n",
      "loss:1.857885 [ 5120/50000]\n",
      "loss:1.738454 [ 5760/50000]\n",
      "loss:1.827636 [ 6400/50000]\n",
      "loss:1.842125 [ 7040/50000]\n",
      "loss:1.809075 [ 7680/50000]\n",
      "loss:1.920492 [ 8320/50000]\n",
      "loss:1.947846 [ 8960/50000]\n",
      "loss:1.883300 [ 9600/50000]\n",
      "loss:1.803014 [10240/50000]\n",
      "loss:1.929793 [10880/50000]\n",
      "loss:1.850451 [11520/50000]\n",
      "loss:1.707066 [12160/50000]\n",
      "loss:1.755239 [12800/50000]\n",
      "loss:1.865385 [13440/50000]\n",
      "loss:1.670236 [14080/50000]\n",
      "loss:1.773468 [14720/50000]\n",
      "loss:1.977683 [15360/50000]\n",
      "loss:1.832226 [16000/50000]\n",
      "loss:1.791228 [16640/50000]\n",
      "loss:1.718826 [17280/50000]\n",
      "loss:1.746141 [17920/50000]\n",
      "loss:1.818312 [18560/50000]\n",
      "loss:1.779544 [19200/50000]\n",
      "loss:1.829920 [19840/50000]\n",
      "loss:1.876570 [20480/50000]\n",
      "loss:1.723041 [21120/50000]\n",
      "loss:1.820628 [21760/50000]\n",
      "loss:1.734087 [22400/50000]\n",
      "loss:1.843263 [23040/50000]\n",
      "loss:1.915064 [23680/50000]\n",
      "loss:1.708047 [24320/50000]\n",
      "loss:1.916201 [24960/50000]\n",
      "loss:1.970044 [25600/50000]\n",
      "loss:1.744183 [26240/50000]\n",
      "loss:1.846062 [26880/50000]\n",
      "loss:1.904410 [27520/50000]\n",
      "loss:2.103059 [28160/50000]\n",
      "loss:1.877727 [28800/50000]\n",
      "loss:1.884219 [29440/50000]\n",
      "loss:1.961709 [30080/50000]\n",
      "loss:1.922898 [30720/50000]\n",
      "loss:1.943402 [31360/50000]\n",
      "loss:1.972706 [32000/50000]\n",
      "loss:1.764987 [32640/50000]\n",
      "loss:1.966479 [33280/50000]\n",
      "loss:1.803533 [33920/50000]\n",
      "loss:1.945772 [34560/50000]\n",
      "loss:1.901245 [35200/50000]\n",
      "loss:1.892228 [35840/50000]\n",
      "loss:1.942252 [36480/50000]\n",
      "loss:1.971523 [37120/50000]\n",
      "loss:1.814758 [37760/50000]\n",
      "loss:1.705536 [38400/50000]\n",
      "loss:1.858091 [39040/50000]\n",
      "loss:1.835244 [39680/50000]\n",
      "loss:1.965279 [40320/50000]\n",
      "loss:1.978431 [40960/50000]\n",
      "loss:1.825776 [41600/50000]\n",
      "loss:1.770779 [42240/50000]\n",
      "loss:1.649413 [42880/50000]\n",
      "loss:1.986781 [43520/50000]\n",
      "loss:1.776002 [44160/50000]\n",
      "loss:1.795121 [44800/50000]\n",
      "loss:1.706534 [45440/50000]\n",
      "loss:1.850700 [46080/50000]\n",
      "loss:1.641018 [46720/50000]\n",
      "loss:1.898441 [47360/50000]\n",
      "loss:1.778935 [48000/50000]\n",
      "loss:1.713029 [48640/50000]\n",
      "loss:1.893685 [49280/50000]\n",
      "loss:1.770242 [49920/50000]\n",
      "test error:\n",
      " accuracy:33.4%,avg loss: 1.823978 \n",
      "\n",
      "epoch 15 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.817511 [    0/50000]\n",
      "loss:1.875423 [  640/50000]\n",
      "loss:1.987538 [ 1280/50000]\n",
      "loss:1.891006 [ 1920/50000]\n",
      "loss:1.741277 [ 2560/50000]\n",
      "loss:1.897165 [ 3200/50000]\n",
      "loss:1.712131 [ 3840/50000]\n",
      "loss:1.855463 [ 4480/50000]\n",
      "loss:2.031552 [ 5120/50000]\n",
      "loss:1.920254 [ 5760/50000]\n",
      "loss:1.817711 [ 6400/50000]\n",
      "loss:1.757757 [ 7040/50000]\n",
      "loss:1.965685 [ 7680/50000]\n",
      "loss:1.831865 [ 8320/50000]\n",
      "loss:1.990008 [ 8960/50000]\n",
      "loss:1.639170 [ 9600/50000]\n",
      "loss:1.569728 [10240/50000]\n",
      "loss:1.624061 [10880/50000]\n",
      "loss:1.795697 [11520/50000]\n",
      "loss:1.664337 [12160/50000]\n",
      "loss:1.603879 [12800/50000]\n",
      "loss:1.860296 [13440/50000]\n",
      "loss:1.849384 [14080/50000]\n",
      "loss:1.721921 [14720/50000]\n",
      "loss:1.764007 [15360/50000]\n",
      "loss:2.014006 [16000/50000]\n",
      "loss:1.840170 [16640/50000]\n",
      "loss:1.735780 [17280/50000]\n",
      "loss:1.861684 [17920/50000]\n",
      "loss:1.606358 [18560/50000]\n",
      "loss:1.868012 [19200/50000]\n",
      "loss:1.882436 [19840/50000]\n",
      "loss:1.650928 [20480/50000]\n",
      "loss:1.834067 [21120/50000]\n",
      "loss:1.652819 [21760/50000]\n",
      "loss:1.721918 [22400/50000]\n",
      "loss:1.684048 [23040/50000]\n",
      "loss:1.950010 [23680/50000]\n",
      "loss:1.980353 [24320/50000]\n",
      "loss:1.693003 [24960/50000]\n",
      "loss:1.807135 [25600/50000]\n",
      "loss:1.721374 [26240/50000]\n",
      "loss:1.842348 [26880/50000]\n",
      "loss:1.986104 [27520/50000]\n",
      "loss:1.705667 [28160/50000]\n",
      "loss:1.827786 [28800/50000]\n",
      "loss:2.059314 [29440/50000]\n",
      "loss:1.834371 [30080/50000]\n",
      "loss:1.631863 [30720/50000]\n",
      "loss:1.888807 [31360/50000]\n",
      "loss:1.775205 [32000/50000]\n",
      "loss:2.010458 [32640/50000]\n",
      "loss:1.911132 [33280/50000]\n",
      "loss:1.770084 [33920/50000]\n",
      "loss:1.743738 [34560/50000]\n",
      "loss:1.809366 [35200/50000]\n",
      "loss:1.851168 [35840/50000]\n",
      "loss:1.750895 [36480/50000]\n",
      "loss:1.773294 [37120/50000]\n",
      "loss:1.794729 [37760/50000]\n",
      "loss:1.729567 [38400/50000]\n",
      "loss:1.730292 [39040/50000]\n",
      "loss:1.734402 [39680/50000]\n",
      "loss:1.740518 [40320/50000]\n",
      "loss:1.719569 [40960/50000]\n",
      "loss:1.886298 [41600/50000]\n",
      "loss:1.782856 [42240/50000]\n",
      "loss:1.909736 [42880/50000]\n",
      "loss:1.633043 [43520/50000]\n",
      "loss:1.704660 [44160/50000]\n",
      "loss:2.017646 [44800/50000]\n",
      "loss:1.859200 [45440/50000]\n",
      "loss:1.771311 [46080/50000]\n",
      "loss:1.756099 [46720/50000]\n",
      "loss:1.769648 [47360/50000]\n",
      "loss:1.672091 [48000/50000]\n",
      "loss:1.809927 [48640/50000]\n",
      "loss:1.719777 [49280/50000]\n",
      "loss:1.597803 [49920/50000]\n",
      "test error:\n",
      " accuracy:35.1%,avg loss: 1.774204 \n",
      "\n",
      "epoch 16 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.680102 [    0/50000]\n",
      "loss:1.755479 [  640/50000]\n",
      "loss:1.663908 [ 1280/50000]\n",
      "loss:1.688250 [ 1920/50000]\n",
      "loss:1.618016 [ 2560/50000]\n",
      "loss:1.860572 [ 3200/50000]\n",
      "loss:1.709356 [ 3840/50000]\n",
      "loss:1.852633 [ 4480/50000]\n",
      "loss:1.878903 [ 5120/50000]\n",
      "loss:1.781487 [ 5760/50000]\n",
      "loss:1.545555 [ 6400/50000]\n",
      "loss:1.813872 [ 7040/50000]\n",
      "loss:1.727852 [ 7680/50000]\n",
      "loss:1.752910 [ 8320/50000]\n",
      "loss:1.903360 [ 8960/50000]\n",
      "loss:1.774606 [ 9600/50000]\n",
      "loss:1.733321 [10240/50000]\n",
      "loss:1.836349 [10880/50000]\n",
      "loss:1.687485 [11520/50000]\n",
      "loss:1.688698 [12160/50000]\n",
      "loss:1.880073 [12800/50000]\n",
      "loss:1.819067 [13440/50000]\n",
      "loss:1.653029 [14080/50000]\n",
      "loss:1.878403 [14720/50000]\n",
      "loss:1.863437 [15360/50000]\n",
      "loss:1.755048 [16000/50000]\n",
      "loss:1.625790 [16640/50000]\n",
      "loss:1.795811 [17280/50000]\n",
      "loss:1.878473 [17920/50000]\n",
      "loss:1.711975 [18560/50000]\n",
      "loss:1.703452 [19200/50000]\n",
      "loss:1.705231 [19840/50000]\n",
      "loss:1.847911 [20480/50000]\n",
      "loss:1.679806 [21120/50000]\n",
      "loss:1.656458 [21760/50000]\n",
      "loss:1.729000 [22400/50000]\n",
      "loss:1.876149 [23040/50000]\n",
      "loss:1.832426 [23680/50000]\n",
      "loss:1.789433 [24320/50000]\n",
      "loss:1.800146 [24960/50000]\n",
      "loss:1.635184 [25600/50000]\n",
      "loss:1.640929 [26240/50000]\n",
      "loss:1.683863 [26880/50000]\n",
      "loss:1.892726 [27520/50000]\n",
      "loss:1.621155 [28160/50000]\n",
      "loss:1.719376 [28800/50000]\n",
      "loss:1.673566 [29440/50000]\n",
      "loss:1.637414 [30080/50000]\n",
      "loss:1.718489 [30720/50000]\n",
      "loss:1.686541 [31360/50000]\n",
      "loss:1.922904 [32000/50000]\n",
      "loss:1.751601 [32640/50000]\n",
      "loss:1.616440 [33280/50000]\n",
      "loss:1.639488 [33920/50000]\n",
      "loss:1.633408 [34560/50000]\n",
      "loss:1.912031 [35200/50000]\n",
      "loss:1.643057 [35840/50000]\n",
      "loss:1.698628 [36480/50000]\n",
      "loss:1.730164 [37120/50000]\n",
      "loss:1.780154 [37760/50000]\n",
      "loss:1.594267 [38400/50000]\n",
      "loss:1.996528 [39040/50000]\n",
      "loss:1.840467 [39680/50000]\n",
      "loss:1.589775 [40320/50000]\n",
      "loss:1.598099 [40960/50000]\n",
      "loss:1.766209 [41600/50000]\n",
      "loss:1.720639 [42240/50000]\n",
      "loss:1.731542 [42880/50000]\n",
      "loss:1.593881 [43520/50000]\n",
      "loss:1.726546 [44160/50000]\n",
      "loss:1.667559 [44800/50000]\n",
      "loss:1.488764 [45440/50000]\n",
      "loss:1.825614 [46080/50000]\n",
      "loss:1.606763 [46720/50000]\n",
      "loss:1.689213 [47360/50000]\n",
      "loss:1.807365 [48000/50000]\n",
      "loss:1.712008 [48640/50000]\n",
      "loss:1.713182 [49280/50000]\n",
      "loss:1.604637 [49920/50000]\n",
      "test error:\n",
      " accuracy:37.2%,avg loss: 1.701638 \n",
      "\n",
      "epoch 17 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.781087 [    0/50000]\n",
      "loss:1.781873 [  640/50000]\n",
      "loss:1.727275 [ 1280/50000]\n",
      "loss:1.567160 [ 1920/50000]\n",
      "loss:1.711388 [ 2560/50000]\n",
      "loss:1.839192 [ 3200/50000]\n",
      "loss:1.932758 [ 3840/50000]\n",
      "loss:1.642175 [ 4480/50000]\n",
      "loss:1.569350 [ 5120/50000]\n",
      "loss:1.919314 [ 5760/50000]\n",
      "loss:1.855617 [ 6400/50000]\n",
      "loss:1.720688 [ 7040/50000]\n",
      "loss:1.564695 [ 7680/50000]\n",
      "loss:1.636160 [ 8320/50000]\n",
      "loss:1.519031 [ 8960/50000]\n",
      "loss:1.703472 [ 9600/50000]\n",
      "loss:1.572216 [10240/50000]\n",
      "loss:1.810792 [10880/50000]\n",
      "loss:1.887872 [11520/50000]\n",
      "loss:1.618398 [12160/50000]\n",
      "loss:2.029837 [12800/50000]\n",
      "loss:1.751820 [13440/50000]\n",
      "loss:1.636333 [14080/50000]\n",
      "loss:1.817220 [14720/50000]\n",
      "loss:1.894838 [15360/50000]\n",
      "loss:1.545888 [16000/50000]\n",
      "loss:1.629736 [16640/50000]\n",
      "loss:1.622036 [17280/50000]\n",
      "loss:1.740874 [17920/50000]\n",
      "loss:1.612728 [18560/50000]\n",
      "loss:1.577376 [19200/50000]\n",
      "loss:1.766300 [19840/50000]\n",
      "loss:1.621432 [20480/50000]\n",
      "loss:1.896239 [21120/50000]\n",
      "loss:1.608137 [21760/50000]\n",
      "loss:1.755979 [22400/50000]\n",
      "loss:1.663388 [23040/50000]\n",
      "loss:1.803348 [23680/50000]\n",
      "loss:1.829908 [24320/50000]\n",
      "loss:1.685096 [24960/50000]\n",
      "loss:1.706060 [25600/50000]\n",
      "loss:1.868949 [26240/50000]\n",
      "loss:1.446717 [26880/50000]\n",
      "loss:1.782554 [27520/50000]\n",
      "loss:1.809118 [28160/50000]\n",
      "loss:1.596316 [28800/50000]\n",
      "loss:1.546015 [29440/50000]\n",
      "loss:1.563321 [30080/50000]\n",
      "loss:1.651902 [30720/50000]\n",
      "loss:1.696183 [31360/50000]\n",
      "loss:1.799802 [32000/50000]\n",
      "loss:1.661558 [32640/50000]\n",
      "loss:1.603718 [33280/50000]\n",
      "loss:1.577795 [33920/50000]\n",
      "loss:1.641151 [34560/50000]\n",
      "loss:1.793081 [35200/50000]\n",
      "loss:1.578345 [35840/50000]\n",
      "loss:1.707589 [36480/50000]\n",
      "loss:1.977940 [37120/50000]\n",
      "loss:1.595405 [37760/50000]\n",
      "loss:1.609851 [38400/50000]\n",
      "loss:1.716345 [39040/50000]\n",
      "loss:1.770660 [39680/50000]\n",
      "loss:1.786894 [40320/50000]\n",
      "loss:1.858873 [40960/50000]\n",
      "loss:1.788370 [41600/50000]\n",
      "loss:1.727023 [42240/50000]\n",
      "loss:1.621791 [42880/50000]\n",
      "loss:1.628958 [43520/50000]\n",
      "loss:1.711634 [44160/50000]\n",
      "loss:1.546625 [44800/50000]\n",
      "loss:1.664918 [45440/50000]\n",
      "loss:1.584361 [46080/50000]\n",
      "loss:1.973413 [46720/50000]\n",
      "loss:1.620763 [47360/50000]\n",
      "loss:1.727412 [48000/50000]\n",
      "loss:1.635930 [48640/50000]\n",
      "loss:1.564127 [49280/50000]\n",
      "loss:1.586287 [49920/50000]\n",
      "test error:\n",
      " accuracy:36.0%,avg loss: 1.765012 \n",
      "\n",
      "epoch 18 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.775043 [    0/50000]\n",
      "loss:1.676141 [  640/50000]\n",
      "loss:1.478803 [ 1280/50000]\n",
      "loss:1.635892 [ 1920/50000]\n",
      "loss:1.785969 [ 2560/50000]\n",
      "loss:1.664522 [ 3200/50000]\n",
      "loss:1.699168 [ 3840/50000]\n",
      "loss:1.709825 [ 4480/50000]\n",
      "loss:1.743038 [ 5120/50000]\n",
      "loss:1.634238 [ 5760/50000]\n",
      "loss:1.612388 [ 6400/50000]\n",
      "loss:1.798856 [ 7040/50000]\n",
      "loss:1.554107 [ 7680/50000]\n",
      "loss:1.690264 [ 8320/50000]\n",
      "loss:1.633385 [ 8960/50000]\n",
      "loss:1.607240 [ 9600/50000]\n",
      "loss:1.518851 [10240/50000]\n",
      "loss:1.674927 [10880/50000]\n",
      "loss:1.687373 [11520/50000]\n",
      "loss:1.524135 [12160/50000]\n",
      "loss:1.565850 [12800/50000]\n",
      "loss:1.776677 [13440/50000]\n",
      "loss:1.757260 [14080/50000]\n",
      "loss:1.429912 [14720/50000]\n",
      "loss:1.506504 [15360/50000]\n",
      "loss:1.606140 [16000/50000]\n",
      "loss:1.858896 [16640/50000]\n",
      "loss:1.706825 [17280/50000]\n",
      "loss:1.596298 [17920/50000]\n",
      "loss:1.568648 [18560/50000]\n",
      "loss:1.851750 [19200/50000]\n",
      "loss:1.588925 [19840/50000]\n",
      "loss:1.873558 [20480/50000]\n",
      "loss:1.761907 [21120/50000]\n",
      "loss:1.652751 [21760/50000]\n",
      "loss:1.616720 [22400/50000]\n",
      "loss:1.749686 [23040/50000]\n",
      "loss:1.700042 [23680/50000]\n",
      "loss:1.601524 [24320/50000]\n",
      "loss:1.723713 [24960/50000]\n",
      "loss:1.617136 [25600/50000]\n",
      "loss:1.429555 [26240/50000]\n",
      "loss:1.575675 [26880/50000]\n",
      "loss:1.690477 [27520/50000]\n",
      "loss:1.600120 [28160/50000]\n",
      "loss:1.558479 [28800/50000]\n",
      "loss:1.667104 [29440/50000]\n",
      "loss:1.658108 [30080/50000]\n",
      "loss:1.581914 [30720/50000]\n",
      "loss:1.806008 [31360/50000]\n",
      "loss:1.670507 [32000/50000]\n",
      "loss:1.502165 [32640/50000]\n",
      "loss:1.696108 [33280/50000]\n",
      "loss:1.655634 [33920/50000]\n",
      "loss:1.636137 [34560/50000]\n",
      "loss:1.853313 [35200/50000]\n",
      "loss:1.847144 [35840/50000]\n",
      "loss:1.660811 [36480/50000]\n",
      "loss:1.637878 [37120/50000]\n",
      "loss:1.860631 [37760/50000]\n",
      "loss:1.790595 [38400/50000]\n",
      "loss:1.736330 [39040/50000]\n",
      "loss:1.670896 [39680/50000]\n",
      "loss:1.646336 [40320/50000]\n",
      "loss:1.666808 [40960/50000]\n",
      "loss:1.584001 [41600/50000]\n",
      "loss:1.735626 [42240/50000]\n",
      "loss:1.660375 [42880/50000]\n",
      "loss:1.794731 [43520/50000]\n",
      "loss:1.643278 [44160/50000]\n",
      "loss:1.612251 [44800/50000]\n",
      "loss:1.754812 [45440/50000]\n",
      "loss:1.843092 [46080/50000]\n",
      "loss:1.420308 [46720/50000]\n",
      "loss:1.644267 [47360/50000]\n",
      "loss:1.624105 [48000/50000]\n",
      "loss:1.536986 [48640/50000]\n",
      "loss:1.809845 [49280/50000]\n",
      "loss:1.464406 [49920/50000]\n",
      "test error:\n",
      " accuracy:38.7%,avg loss: 1.664742 \n",
      "\n",
      "epoch 19 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.574249 [    0/50000]\n",
      "loss:1.685791 [  640/50000]\n",
      "loss:1.682337 [ 1280/50000]\n",
      "loss:1.733506 [ 1920/50000]\n",
      "loss:1.649398 [ 2560/50000]\n",
      "loss:1.655362 [ 3200/50000]\n",
      "loss:1.728075 [ 3840/50000]\n",
      "loss:1.691660 [ 4480/50000]\n",
      "loss:1.524124 [ 5120/50000]\n",
      "loss:1.616129 [ 5760/50000]\n",
      "loss:1.661180 [ 6400/50000]\n",
      "loss:1.552425 [ 7040/50000]\n",
      "loss:1.618830 [ 7680/50000]\n",
      "loss:1.668211 [ 8320/50000]\n",
      "loss:1.629336 [ 8960/50000]\n",
      "loss:1.640664 [ 9600/50000]\n",
      "loss:1.616091 [10240/50000]\n",
      "loss:1.578896 [10880/50000]\n",
      "loss:1.685213 [11520/50000]\n",
      "loss:1.502913 [12160/50000]\n",
      "loss:1.666624 [12800/50000]\n",
      "loss:1.559861 [13440/50000]\n",
      "loss:1.622694 [14080/50000]\n",
      "loss:1.657756 [14720/50000]\n",
      "loss:1.786098 [15360/50000]\n",
      "loss:1.656965 [16000/50000]\n",
      "loss:1.677704 [16640/50000]\n",
      "loss:1.513270 [17280/50000]\n",
      "loss:1.697211 [17920/50000]\n",
      "loss:1.652592 [18560/50000]\n",
      "loss:1.553873 [19200/50000]\n",
      "loss:1.464055 [19840/50000]\n",
      "loss:1.762569 [20480/50000]\n",
      "loss:1.633327 [21120/50000]\n",
      "loss:1.551096 [21760/50000]\n",
      "loss:1.736086 [22400/50000]\n",
      "loss:1.540488 [23040/50000]\n",
      "loss:1.846526 [23680/50000]\n",
      "loss:1.667165 [24320/50000]\n",
      "loss:1.662212 [24960/50000]\n",
      "loss:1.682074 [25600/50000]\n",
      "loss:1.659783 [26240/50000]\n",
      "loss:1.532134 [26880/50000]\n",
      "loss:1.667906 [27520/50000]\n",
      "loss:1.768588 [28160/50000]\n",
      "loss:1.618178 [28800/50000]\n",
      "loss:1.460470 [29440/50000]\n",
      "loss:1.438762 [30080/50000]\n",
      "loss:1.746462 [30720/50000]\n",
      "loss:1.512219 [31360/50000]\n",
      "loss:1.624343 [32000/50000]\n",
      "loss:1.528677 [32640/50000]\n",
      "loss:1.946404 [33280/50000]\n",
      "loss:1.643192 [33920/50000]\n",
      "loss:1.780127 [34560/50000]\n",
      "loss:1.652503 [35200/50000]\n",
      "loss:1.553724 [35840/50000]\n",
      "loss:1.550487 [36480/50000]\n",
      "loss:1.582986 [37120/50000]\n",
      "loss:1.688684 [37760/50000]\n",
      "loss:1.511981 [38400/50000]\n",
      "loss:1.543475 [39040/50000]\n",
      "loss:1.570854 [39680/50000]\n",
      "loss:1.736522 [40320/50000]\n",
      "loss:1.533348 [40960/50000]\n",
      "loss:1.533575 [41600/50000]\n",
      "loss:1.560163 [42240/50000]\n",
      "loss:1.797407 [42880/50000]\n",
      "loss:1.394069 [43520/50000]\n",
      "loss:1.686348 [44160/50000]\n",
      "loss:1.632182 [44800/50000]\n",
      "loss:1.718711 [45440/50000]\n",
      "loss:1.548339 [46080/50000]\n",
      "loss:1.614141 [46720/50000]\n",
      "loss:1.689536 [47360/50000]\n",
      "loss:1.632068 [48000/50000]\n",
      "loss:1.678737 [48640/50000]\n",
      "loss:1.599739 [49280/50000]\n",
      "loss:1.584930 [49920/50000]\n",
      "test error:\n",
      " accuracy:38.3%,avg loss: 1.724304 \n",
      "\n",
      "epoch 20 \n",
      "----------------------------------------------\n",
      "training on:cuda\n",
      "loss:1.831363 [    0/50000]\n",
      "loss:1.596744 [  640/50000]\n",
      "loss:1.539291 [ 1280/50000]\n",
      "loss:1.569760 [ 1920/50000]\n",
      "loss:1.769176 [ 2560/50000]\n",
      "loss:1.611403 [ 3200/50000]\n",
      "loss:1.465447 [ 3840/50000]\n",
      "loss:1.616200 [ 4480/50000]\n",
      "loss:1.934613 [ 5120/50000]\n",
      "loss:1.540497 [ 5760/50000]\n",
      "loss:1.631879 [ 6400/50000]\n",
      "loss:1.523600 [ 7040/50000]\n",
      "loss:1.608161 [ 7680/50000]\n",
      "loss:1.591281 [ 8320/50000]\n",
      "loss:1.712337 [ 8960/50000]\n",
      "loss:1.612620 [ 9600/50000]\n",
      "loss:1.639016 [10240/50000]\n",
      "loss:1.422774 [10880/50000]\n",
      "loss:1.519030 [11520/50000]\n",
      "loss:1.642595 [12160/50000]\n",
      "loss:1.664353 [12800/50000]\n",
      "loss:1.369121 [13440/50000]\n",
      "loss:1.686282 [14080/50000]\n",
      "loss:1.483930 [14720/50000]\n",
      "loss:1.481691 [15360/50000]\n",
      "loss:1.586711 [16000/50000]\n",
      "loss:1.582291 [16640/50000]\n",
      "loss:1.492859 [17280/50000]\n",
      "loss:1.422312 [17920/50000]\n",
      "loss:1.601753 [18560/50000]\n",
      "loss:1.736949 [19200/50000]\n",
      "loss:1.927687 [19840/50000]\n",
      "loss:1.599930 [20480/50000]\n",
      "loss:1.532207 [21120/50000]\n",
      "loss:1.509454 [21760/50000]\n",
      "loss:1.707057 [22400/50000]\n",
      "loss:1.630521 [23040/50000]\n",
      "loss:1.580653 [23680/50000]\n",
      "loss:1.550992 [24320/50000]\n",
      "loss:1.561492 [24960/50000]\n",
      "loss:1.549258 [25600/50000]\n",
      "loss:1.466412 [26240/50000]\n",
      "loss:1.558691 [26880/50000]\n",
      "loss:1.599306 [27520/50000]\n",
      "loss:1.532681 [28160/50000]\n",
      "loss:1.631280 [28800/50000]\n",
      "loss:1.691051 [29440/50000]\n",
      "loss:1.789497 [30080/50000]\n",
      "loss:1.807231 [30720/50000]\n",
      "loss:1.786741 [31360/50000]\n",
      "loss:1.806576 [32000/50000]\n",
      "loss:1.756900 [32640/50000]\n",
      "loss:1.519877 [33280/50000]\n",
      "loss:1.743891 [33920/50000]\n",
      "loss:1.618176 [34560/50000]\n",
      "loss:1.696581 [35200/50000]\n",
      "loss:1.537158 [35840/50000]\n",
      "loss:1.573044 [36480/50000]\n",
      "loss:1.481097 [37120/50000]\n",
      "loss:1.726101 [37760/50000]\n",
      "loss:1.684310 [38400/50000]\n",
      "loss:1.542842 [39040/50000]\n",
      "loss:1.800000 [39680/50000]\n",
      "loss:1.604882 [40320/50000]\n",
      "loss:1.537045 [40960/50000]\n",
      "loss:1.764459 [41600/50000]\n",
      "loss:1.860248 [42240/50000]\n",
      "loss:1.663251 [42880/50000]\n",
      "loss:1.714696 [43520/50000]\n",
      "loss:1.699913 [44160/50000]\n",
      "loss:1.758045 [44800/50000]\n",
      "loss:1.572754 [45440/50000]\n",
      "loss:1.751402 [46080/50000]\n",
      "loss:1.795436 [46720/50000]\n",
      "loss:1.639807 [47360/50000]\n",
      "loss:1.625308 [48000/50000]\n",
      "loss:1.484860 [48640/50000]\n",
      "loss:1.813962 [49280/50000]\n",
      "loss:1.475419 [49920/50000]\n",
      "test error:\n",
      " accuracy:39.6%,avg loss: 1.649022 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(epoch):\n",
    "    print(f\"epoch {t+1} \\n----------------------------------------------\")\n",
    "    train(trainLoader,model,loss,optimizer)\n",
    "    max_correct,correct=test(testLoader,model,loss,max_correct)\n",
    "    writer.add_scalar(\"accuracy\",correct,t)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
