{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-03T07:43:40.975599600Z",
     "start_time": "2024-03-03T07:43:38.828963200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "from AlexNet import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# net=nn.Sequential(\n",
    "#     nn.Conv2d(1,96,kernel_size=11,stride=4,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "#\n",
    "#     nn.Conv2d(96, 256, kernel_size=5,  padding=2),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#\n",
    "#     nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "#\n",
    "#     nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "#\n",
    "#     nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "#\n",
    "#     nn.Flatten(),\n",
    "#\n",
    "#     nn.Linear(256*5*5,4096),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#\n",
    "#     nn.Linear(4096,4096),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#\n",
    "#     nn.Linear(4096,10)\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T07:43:40.980207400Z",
     "start_time": "2024-03-03T07:43:40.980207400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# X=torch.randn(1,1,224,224)\n",
    "# for layer in net:\n",
    "#     X=layer(X)\n",
    "#     print(layer.__class__.__name__,\"output shape:\\t\",X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T07:43:40.985081100Z",
     "start_time": "2024-03-03T07:43:40.980207400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    Resize([224,224]),\n",
    "    ToTensor()\n",
    "])\n",
    "mnist_training = datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "mnist_test = datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T07:43:41.029491200Z",
     "start_time": "2024-03-03T07:43:40.988590300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ../data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=[224, 224], interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           ) Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=[224, 224], interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(mnist_test,mnist_training)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T07:43:41.038835Z",
     "start_time": "2024-03-03T07:43:41.029491200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache ()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T07:43:41.089622500Z",
     "start_time": "2024-03-03T07:43:41.038835Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "\n",
    "train_dataloader = DataLoader(mnist_training, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T07:43:41.089622500Z",
     "start_time": "2024-03-03T07:43:41.063698700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \n",
      "---------------------\n",
      "loss:2.305946 [    0/ 60000]\n",
      "loss:2.294373 [ 2560/ 60000]\n",
      "loss:2.279762 [ 5120/ 60000]\n",
      "loss:2.211135 [ 7680/ 60000]\n",
      "loss:1.486849 [10240/ 60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 17\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m---------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     14\u001B[0m )\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, (inputs, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_dataloader):\n\u001B[1;32m---> 17\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     19\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     21\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m net(inputs)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "net=AlexNet().to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "net.apply(init_weights)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\n",
    "        f\"epoch {epoch} \\n---------------------\"\n",
    "    )\n",
    "\n",
    "    for batch, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(inputs)\n",
    "            print(f\"loss:{loss:>7f} [{current:>5d}/ 60000]\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc = 0\n",
    "        total = 0\n",
    "        for (image, label) in test_dataloader:\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            output = net(image)\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total += label.size(0)\n",
    "            acc += (pred == label).sum()\n",
    "\n",
    "        print(f\"test: acc {100 * acc / total}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T07:43:58.701021200Z",
     "start_time": "2024-03-03T07:43:41.089622500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pytorch",
   "language": "python",
   "display_name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
