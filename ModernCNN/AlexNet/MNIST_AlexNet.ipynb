{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-01T07:10:27.792188400Z",
     "start_time": "2024-03-01T07:10:25.446231300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "from AlexNet import AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# net=nn.Sequential(\n",
    "#     nn.Conv2d(1,96,kernel_size=11,stride=4,padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "#\n",
    "#     nn.Conv2d(96, 256, kernel_size=5,  padding=2),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "#\n",
    "#     nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "#\n",
    "#     nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "#\n",
    "#     nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "#     nn.ReLU(),\n",
    "#     nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "#\n",
    "#     nn.Flatten(),\n",
    "#\n",
    "#     nn.Linear(256*5*5,4096),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#\n",
    "#     nn.Linear(4096,4096),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#\n",
    "#     nn.Linear(4096,10)\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T07:10:27.797664300Z",
     "start_time": "2024-03-01T07:10:27.795159700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# X=torch.randn(1,1,224,224)\n",
    "# for layer in net:\n",
    "#     X=layer(X)\n",
    "#     print(layer.__class__.__name__,\"output shape:\\t\",X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T07:10:27.803501800Z",
     "start_time": "2024-03-01T07:10:27.798661100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    Resize([224,224]),\n",
    "    ToTensor()\n",
    "])\n",
    "mnist_training = datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "mnist_test = datasets.MNIST(\n",
    "    root=\"../data\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T07:10:27.852088900Z",
     "start_time": "2024-03-01T07:10:27.803501800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ../data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=[224, 224], interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           ) Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=[224, 224], interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(mnist_test,mnist_training)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T07:10:27.852088900Z",
     "start_time": "2024-03-01T07:10:27.848023700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache ()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T07:10:27.905395900Z",
     "start_time": "2024-03-01T07:10:27.854082800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "\n",
    "train_dataloader = DataLoader(mnist_training, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T07:10:27.914372600Z",
     "start_time": "2024-03-01T07:10:27.883198500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 \n",
      "---------------------\n",
      "loss:2.309132 [    0/ 60000]\n",
      "loss:2.296461 [ 2560/ 60000]\n",
      "loss:2.275908 [ 5120/ 60000]\n",
      "loss:2.206079 [ 7680/ 60000]\n",
      "loss:1.562011 [10240/ 60000]\n",
      "loss:1.082462 [12800/ 60000]\n",
      "loss:0.446683 [15360/ 60000]\n",
      "loss:0.363827 [17920/ 60000]\n",
      "loss:0.233036 [20480/ 60000]\n",
      "loss:0.254791 [23040/ 60000]\n",
      "loss:0.201992 [25600/ 60000]\n",
      "loss:0.255451 [28160/ 60000]\n",
      "loss:0.180451 [30720/ 60000]\n",
      "loss:0.102079 [33280/ 60000]\n",
      "loss:0.107142 [35840/ 60000]\n",
      "loss:0.125804 [38400/ 60000]\n",
      "loss:0.104566 [40960/ 60000]\n",
      "loss:0.123582 [43520/ 60000]\n",
      "loss:0.084459 [46080/ 60000]\n",
      "loss:0.113914 [48640/ 60000]\n",
      "loss:0.151553 [51200/ 60000]\n",
      "loss:0.101548 [53760/ 60000]\n",
      "loss:0.102417 [56320/ 60000]\n",
      "loss:0.086216 [58880/ 60000]\n",
      "test: acc 97.63999938964844\n",
      "epoch 1 \n",
      "---------------------\n",
      "loss:0.067266 [    0/ 60000]\n",
      "loss:0.113348 [ 2560/ 60000]\n",
      "loss:0.123024 [ 5120/ 60000]\n",
      "loss:0.104231 [ 7680/ 60000]\n",
      "loss:0.094467 [10240/ 60000]\n",
      "loss:0.055352 [12800/ 60000]\n",
      "loss:0.101548 [15360/ 60000]\n",
      "loss:0.051812 [17920/ 60000]\n",
      "loss:0.074520 [20480/ 60000]\n",
      "loss:0.048130 [23040/ 60000]\n",
      "loss:0.030063 [25600/ 60000]\n",
      "loss:0.107249 [28160/ 60000]\n",
      "loss:0.097766 [30720/ 60000]\n",
      "loss:0.036431 [33280/ 60000]\n",
      "loss:0.033842 [35840/ 60000]\n",
      "loss:0.075659 [38400/ 60000]\n",
      "loss:0.057449 [40960/ 60000]\n",
      "loss:0.042159 [43520/ 60000]\n",
      "loss:0.114527 [46080/ 60000]\n",
      "loss:0.051474 [48640/ 60000]\n",
      "loss:0.018162 [51200/ 60000]\n",
      "loss:0.025694 [53760/ 60000]\n",
      "loss:0.036466 [56320/ 60000]\n",
      "loss:0.108626 [58880/ 60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net=AlexNet().to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "net.apply(init_weights)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\n",
    "        f\"epoch {epoch} \\n---------------------\"\n",
    "    )\n",
    "\n",
    "    for batch, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(inputs)\n",
    "            print(f\"loss:{loss:>7f} [{current:>5d}/ 60000]\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc = 0\n",
    "        total = 0\n",
    "        for (image, label) in test_dataloader:\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            output = net(image)\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total += label.size(0)\n",
    "            acc += (pred == label).sum()\n",
    "\n",
    "        print(f\"test: acc {100 * acc / total}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T07:13:09.707433200Z",
     "start_time": "2024-03-01T07:10:27.905395900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
